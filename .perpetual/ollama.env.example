# Example .env config, version: development

# Options for Ollama instance, local or public.
# When using a large enough model it can produce good results for some operations. See docs/ollama.md for more info

# Configuration files should have ".env" extensions and it can be placed to the following locations:
# Project local config: <Project root>/.perpetual/*.env
# Global config. On Linux: ~/.config/Perpetual/*.env ; On Windows: <User profile dir>\AppData\Roaming\Perpetual\*.env
# Also, the parameters can be exported to the system environment before running the utility, then they will have priority over the parameters in the configuration files. The "*.env" files will be loaded in alphabetical order, with parameters in previously loaded files taking precedence.

# Uncomment if this is the only .env config file you are using
# LLM_PROVIDER="ollama"

# OLLAMA_BASE_URL="http://127.0.0.1:11434"

# Optional authentication, for use with external https proxy or public instance
# OLLAMA_AUTH_TYPE="Bearer" # Type of the authentication used, "Bearer" - api key or token (default), "Basic" - web auth with login and password.
# OLLAMA_AUTH="<your api-key or token goes here>" # When using bearer auth type, put your api key or auth token here
# OLLAMA_AUTH="<login>:<password>" # Web auth requres login and password separated by a colon

OLLAMA_MODEL_OP_ANNOTATE="qwen2.5-coder:7b-instruct-q5_K_M"
OLLAMA_MODEL_OP_ANNOTATE_POST="qwen2.5-coder:7b-instruct-q5_K_M" # used to process multiple response-variants if any
# OLLAMA_MODEL_OP_EMBED="snowflake-arctic-embed2" # uncomment to enable embedding, install model by running ollama pull snowflake-arctic-embed2
# OLLAMA_MODEL_OP_IMPLEMENT_STAGE1="qwen2.5-coder:14b-instruct-q4_K_M"
# OLLAMA_MODEL_OP_IMPLEMENT_STAGE2="qwen2.5-coder:14b-instruct-q4_K_M"
# OLLAMA_MODEL_OP_IMPLEMENT_STAGE3="qwen2.5-coder:14b-instruct-q4_K_M"
# OLLAMA_MODEL_OP_IMPLEMENT_STAGE4="qwen2.5-coder:14b-instruct-q4_K_M"
# OLLAMA_MODEL_OP_DOC_STAGE1="llama3.1:70b-instruct-q4_K_S"
# OLLAMA_MODEL_OP_DOC_STAGE2="llama3.1:70b-instruct-q4_K_S"
# OLLAMA_MODEL_OP_EXPLAIN_STAGE1="llama3.1:70b-instruct-q4_K_S"
# OLLAMA_MODEL_OP_EXPLAIN_STAGE2="llama3.1:70b-instruct-q4_K_S"
OLLAMA_MODEL="qwen2.5-coder:14b-instruct-q4_K_M"
OLLAMA_VARIANT_COUNT_OP_ANNOTATE="1" # how much annotate-response variants to generate
OLLAMA_VARIANT_SELECTION_OP_ANNOTATE="short" # how to select final variant: short, long, combine, best
OLLAMA_VARIANT_COUNT="1" # will be used as fallback
OLLAMA_VARIANT_SELECTION="short" # will be used as fallback

# Text chunk/sequence size in characters (not tokens), used when generating embeddings.
# Optimal values are model dependent, large values may overflow model context window. Example below is for snowflake-arctic-embed2
# Values too small or too large may lead to less effective search or may not work at all.
# OLLAMA_EMBED_DOC_CHUNK_SIZE="1024"
# OLLAMA_EMBED_DOC_CHUNK_OVERLAP="64"
# OLLAMA_EMBED_SEARCH_CHUNK_SIZE="4096"
# OLLAMA_EMBED_SEARCH_CHUNK_OVERLAP="128"

# Cosine score threshold value to consider search vector simiar to target, usually not need to change anything here.
# Model dependent, may be less than 0 for some models, (score < 0 usually means that the vectors are semantically opposite)
# OLLAMA_EMBED_SCORE_THRESHOLD="0.0"

# LLM query text prefix used when generating embeddings for project files. Model dependent. Unset by default.
# OLLAMA_EMBED_DOC_PREFIX="Process following document:\n"
# LLM query text prefix used when generating embeddings for search queries. Model dependent. Unset by default.
# OLLAMA_EMBED_SEARCH_PREFIX="Process following search query:\n"

# examples for some models:
# nomic-embed-text-v1.5
# OLLAMA_EMBED_DOC_PREFIX="search_document: \n"
# OLLAMA_EMBED_SEARCH_PREFIX="search_query: \n"
# mxbai-embed-large-v1
# OLLAMA_EMBED_DOC_PREFIX=""
# OLLAMA_EMBED_SEARCH_PREFIX="Represent this sentence for searching relevant passages: \n"

# Context overflow detection and management options: multiplier to increase context size, upper limit, and estimation multiplier.
# Used to detect overflow and increase context size on errors if needed. Context size will reset back to initial value when starting new operation stage.
# These params may be removed in future when Ollama implement API calls for tokenizer or prompt size statistics
OLLAMA_CONTEXT_MULT="1.75"
OLLAMA_CONTEXT_SIZE_LIMIT="49152" # reasonable limit for typical desktop systems with 32G of RAM and 8G of VRAM
OLLAMA_CONTEXT_ESTIMATE_MULT="0.3"

# Context window sizes for different operations, if set too low, it will be extended automatically when context overflow detected
# If not set - use default for ollama model, and also disable context overflow detection above
OLLAMA_CONTEXT_SIZE_OP_ANNOTATE="6144"
OLLAMA_CONTEXT_SIZE_OP_ANNOTATE_POST="6144"
OLLAMA_CONTEXT_SIZE_OP_IMPLEMENT_STAGE1="24576"
OLLAMA_CONTEXT_SIZE_OP_IMPLEMENT_STAGE2="24576"
OLLAMA_CONTEXT_SIZE_OP_IMPLEMENT_STAGE3="24576"
OLLAMA_CONTEXT_SIZE_OP_IMPLEMENT_STAGE4="24576"
OLLAMA_CONTEXT_SIZE_OP_DOC_STAGE1="49152"
OLLAMA_CONTEXT_SIZE_OP_DOC_STAGE2="49152"
OLLAMA_CONTEXT_SIZE_OP_EXPLAIN_STAGE1="24576"
OLLAMA_CONTEXT_SIZE_OP_EXPLAIN_STAGE2="24576"
OLLAMA_CONTEXT_SIZE="24576"

# Switch to use structured JSON output format for some operations, may work better with some models (or not work at all)
# Supported values: plain, json. Default: plain
OLLAMA_FORMAT_OP_IMPLEMENT_STAGE1="json"
OLLAMA_FORMAT_OP_IMPLEMENT_STAGE3="json"
OLLAMA_FORMAT_OP_DOC_STAGE1="json"
OLLAMA_FORMAT_OP_EXPLAIN_STAGE1="json"

OLLAMA_MAX_TOKENS_OP_ANNOTATE="768"
OLLAMA_MAX_TOKENS_OP_ANNOTATE_POST="768"
OLLAMA_MAX_TOKENS_OP_IMPLEMENT_STAGE1="512" # file-list for review, long list is probably an error
OLLAMA_MAX_TOKENS_OP_IMPLEMENT_STAGE2="1536" # work plan also should not be too big
OLLAMA_MAX_TOKENS_OP_IMPLEMENT_STAGE3="512" # file-list for processing, long list is probably an error
OLLAMA_MAX_TOKENS_OP_IMPLEMENT_STAGE4="4096" # generated code output limit should be as big as possible
OLLAMA_MAX_TOKENS_OP_DOC_STAGE1="768" # file-list for review, long list is probably an error
OLLAMA_MAX_TOKENS_OP_DOC_STAGE2="4096" # generated document output limit should be as big as possible
OLLAMA_MAX_TOKENS_OP_EXPLAIN_STAGE1="512" # file-list for review
OLLAMA_MAX_TOKENS_OP_EXPLAIN_STAGE2="8192" # generated answer output limit
OLLAMA_MAX_TOKENS="4096"
OLLAMA_MAX_TOKENS_SEGMENTS="3"
OLLAMA_ON_FAIL_RETRIES_OP_ANNOTATE="5" # this number include errors caused by context overflow
# OLLAMA_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE1="3"
# OLLAMA_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE2="3"
# OLLAMA_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE3="3"
# OLLAMA_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE4="3"
# OLLAMA_ON_FAIL_RETRIES_OP_DOC_STAGE1="3"
# OLLAMA_ON_FAIL_RETRIES_OP_DOC_STAGE2="3"
# OLLAMA_ON_FAIL_RETRIES_OP_EXPLAIN_STAGE1="3"
# OLLAMA_ON_FAIL_RETRIES_OP_EXPLAIN_STAGE2="3"
OLLAMA_ON_FAIL_RETRIES="3"
# note: temperature highly depends on model, 0 produces mostly deterministic results
# OLLAMA_TEMPERATURE_OP_ANNOTATE="0.5"
# OLLAMA_TEMPERATURE_OP_ANNOTATE_POST="0"
# OLLAMA_TEMPERATURE_OP_IMPLEMENT_STAGE1="0.2" # less creative for file-list output
# OLLAMA_TEMPERATURE_OP_IMPLEMENT_STAGE2="0.5"
# OLLAMA_TEMPERATURE_OP_IMPLEMENT_STAGE3="0.2" # less creative for file-list output
# OLLAMA_TEMPERATURE_OP_IMPLEMENT_STAGE4="0.5"
# OLLAMA_TEMPERATURE_OP_DOC_STAGE1="0.2" # less creative for file-list output
# OLLAMA_TEMPERATURE_OP_DOC_STAGE2="0.7"
# OLLAMA_TEMPERATURE_OP_EXPLAIN_STAGE1="0.2" # less creative for file-list output
# OLLAMA_TEMPERATURE_OP_EXPLAIN_STAGE2="0.7"
# OLLAMA_TEMPERATURE="0.5"

# System prompt role for model, can be configured per operation. Useful if model not supporting system prompt
# Valid values: system, user. default: system.
# When using "user" role, system prompt will be converted to user-query + ai-acknowledge message sequence
# OLLAMA_SYSPROMPT_ROLE_OP_ANNOTATE="system"
# OLLAMA_SYSPROMPT_ROLE_OP_ANNOTATE_POST="system"
# OLLAMA_SYSPROMPT_ROLE_OP_IMPLEMENT_STAGE1="system"
# OLLAMA_SYSPROMPT_ROLE_OP_IMPLEMENT_STAGE2="system"
# OLLAMA_SYSPROMPT_ROLE_OP_IMPLEMENT_STAGE3="system"
# OLLAMA_SYSPROMPT_ROLE_OP_IMPLEMENT_STAGE4="system"
# OLLAMA_SYSPROMPT_ROLE_OP_DOC_STAGE1="system"
# OLLAMA_SYSPROMPT_ROLE_OP_DOC_STAGE2="system"
OLLAMA_SYSPROMPT_ROLE_OP_EXPLAIN_STAGE1="system"
OLLAMA_SYSPROMPT_ROLE_OP_EXPLAIN_STAGE2="system"
# OLLAMA_SYSPROMPT_ROLE="system"

# Optional system- and user- prompt prefixes and suffixes.
# You may need to use it with models like Qwen3 to switch between reasoning / non-reasoning modes
# Or to perform some other model-specific fine-tuning, example below is for Qwen3
# OLLAMA_SYSTEM_PFX_OP_ANNOTATE=""
# OLLAMA_SYSTEM_SFX_OP_ANNOTATE=" /no_think"
# OLLAMA_USER_PFX_OP_ANNOTATE=""
# OLLAMA_USER_SFX_OP_ANNOTATE=" /no_think"
# OLLAMA_SYSTEM_PFX_OP_ANNOTATE_POST=""
# OLLAMA_SYSTEM_SFX_OP_ANNOTATE_POST=" /no_think"
# OLLAMA_USER_PFX_OP_ANNOTATE_POST=""
# OLLAMA_USER_SFX_OP_ANNOTATE_POST=" /no_think"
# OLLAMA_SYSTEM_PFX_OP_IMPLEMENT_STAGE1=""
# OLLAMA_SYSTEM_SFX_OP_IMPLEMENT_STAGE1=" /no_think"
# OLLAMA_USER_PFX_OP_IMPLEMENT_STAGE1=""
# OLLAMA_USER_SFX_OP_IMPLEMENT_STAGE1=" /no_think"
# OLLAMA_SYSTEM_PFX_OP_IMPLEMENT_STAGE2=""
# OLLAMA_SYSTEM_SFX_OP_IMPLEMENT_STAGE2=" /no_think"
# OLLAMA_USER_PFX_OP_IMPLEMENT_STAGE2=""
# OLLAMA_USER_SFX_OP_IMPLEMENT_STAGE2=" /no_think"
# OLLAMA_SYSTEM_PFX_OP_IMPLEMENT_STAGE3=""
# OLLAMA_SYSTEM_SFX_OP_IMPLEMENT_STAGE3=" /no_think"
# OLLAMA_USER_PFX_OP_IMPLEMENT_STAGE3=""
# OLLAMA_USER_SFX_OP_IMPLEMENT_STAGE3=" /no_think"
# OLLAMA_SYSTEM_PFX_OP_IMPLEMENT_STAGE4=""
# OLLAMA_SYSTEM_SFX_OP_IMPLEMENT_STAGE4=" /no_think"
# OLLAMA_USER_PFX_OP_IMPLEMENT_STAGE4=""
# OLLAMA_USER_SFX_OP_IMPLEMENT_STAGE4=" /no_think"
# OLLAMA_SYSTEM_PFX_OP_DOC_STAGE1=""
# OLLAMA_SYSTEM_SFX_OP_DOC_STAGE1=" /no_think"
# OLLAMA_USER_PFX_OP_DOC_STAGE1=""
# OLLAMA_USER_SFX_OP_DOC_STAGE1=" /no_think"
# OLLAMA_SYSTEM_PFX_OP_DOC_STAGE2=""
# OLLAMA_SYSTEM_SFX_OP_DOC_STAGE2=" /no_think"
# OLLAMA_USER_PFX_OP_DOC_STAGE2=""
# OLLAMA_USER_SFX_OP_DOC_STAGE2=" /no_think"
# OLLAMA_SYSTEM_PFX_OP_EXPLAIN_STAGE1=""
# OLLAMA_SYSTEM_SFX_OP_EXPLAIN_STAGE1=" /no_think"
# OLLAMA_USER_PFX_OP_EXPLAIN_STAGE1=""
# OLLAMA_USER_SFX_OP_EXPLAIN_STAGE1=" /no_think"
# OLLAMA_SYSTEM_PFX_OP_EXPLAIN_STAGE2=""
# OLLAMA_SYSTEM_SFX_OP_EXPLAIN_STAGE2=" /no_think"
# OLLAMA_USER_PFX_OP_EXPLAIN_STAGE2=""
# OLLAMA_USER_SFX_OP_EXPLAIN_STAGE2=" /no_think"
# OLLAMA_SYSTEM_PFX=""
# OLLAMA_SYSTEM_SFX=" /no_think"
# OLLAMA_USER_PFX=""
# OLLAMA_USER_SFX=" /no_think"

# Optional regexps for filtering out responses from reasoning models, like deepseek r1 or qwen3
# THINK-regexps will be used to remove reasoning part from response L - is for opening tag, R - is for closing tag
# OUT-regexps will be used to extract output part from response after it was filered out with THINK-regexps
# OLLAMA_THINK_RX_L_OP_ANNOTATE="(?mi)^\\s*<think>\\s*\$"
# OLLAMA_THINK_RX_R_OP_ANNOTATE="(?mi)^\\s*</think>\\s*\$"
# OLLAMA_OUT_RX_L_OP_ANNOTATE="(?mi)^\\s*<output>\\s*\$"
# OLLAMA_OUT_RX_R_OP_ANNOTATE="(?mi)^\\s*</output>\\s*\$"
# OLLAMA_THINK_RX_L_OP_ANNOTATE_POST="(?mi)^\\s*<think>\\s*\$"
# OLLAMA_THINK_RX_R_OP_ANNOTATE_POST="(?mi)^\\s*</think>\\s*\$"
# OLLAMA_OUT_RX_L_OP_ANNOTATE_POST="(?mi)^\\s*<output>\\s*\$"
# OLLAMA_OUT_RX_R_OP_ANNOTATE_POST="(?mi)^\\s*</output>\\s*\$"
# OLLAMA_THINK_RX_L_OP_IMPLEMENT_STAGE1="(?mi)^\\s*<think>\\s*\$"
# OLLAMA_THINK_RX_R_OP_IMPLEMENT_STAGE1="(?mi)^\\s*</think>\\s*\$"
# OLLAMA_OUT_RX_L_OP_IMPLEMENT_STAGE1="(?mi)^\\s*<output>\\s*\$"
# OLLAMA_OUT_RX_R_OP_IMPLEMENT_STAGE1="(?mi)^\\s*</output>\\s*\$"
# OLLAMA_THINK_RX_L_OP_IMPLEMENT_STAGE2="(?mi)^\\s*<think>\\s*\$"
# OLLAMA_THINK_RX_R_OP_IMPLEMENT_STAGE2="(?mi)^\\s*</think>\\s*\$"
# OLLAMA_OUT_RX_L_OP_IMPLEMENT_STAGE2="(?mi)^\\s*<output>\\s*\$"
# OLLAMA_OUT_RX_R_OP_IMPLEMENT_STAGE2="(?mi)^\\s*</output>\\s*\$"
# OLLAMA_THINK_RX_L_OP_IMPLEMENT_STAGE3="(?mi)^\\s*<think>\\s*\$"
# OLLAMA_THINK_RX_R_OP_IMPLEMENT_STAGE3="(?mi)^\\s*</think>\\s*\$"
# OLLAMA_OUT_RX_L_OP_IMPLEMENT_STAGE3="(?mi)^\\s*<output>\\s*\$"
# OLLAMA_OUT_RX_R_OP_IMPLEMENT_STAGE3="(?mi)^\\s*</output>\\s*\$"
# OLLAMA_THINK_RX_L_OP_IMPLEMENT_STAGE4="(?mi)^\\s*<think>\\s*\$"
# OLLAMA_THINK_RX_R_OP_IMPLEMENT_STAGE4="(?mi)^\\s*</think>\\s*\$"
# OLLAMA_OUT_RX_L_OP_IMPLEMENT_STAGE4="(?mi)^\\s*<output>\\s*\$"
# OLLAMA_OUT_RX_R_OP_IMPLEMENT_STAGE4="(?mi)^\\s*</output>\\s*\$"
# OLLAMA_THINK_RX_L_OP_DOC_STAGE1="(?mi)^\\s*<think>\\s*\$"
# OLLAMA_THINK_RX_R_OP_DOC_STAGE1="(?mi)^\\s*</think>\\s*\$"
# OLLAMA_OUT_RX_L_OP_DOC_STAGE1="(?mi)^\\s*<output>\\s*\$"
# OLLAMA_OUT_RX_R_OP_DOC_STAGE1="(?mi)^\\s*</output>\\s*\$"
# OLLAMA_THINK_RX_L_OP_DOC_STAGE2="(?mi)^\\s*<think>\\s*\$"
# OLLAMA_THINK_RX_R_OP_DOC_STAGE2="(?mi)^\\s*</think>\\s*\$"
# OLLAMA_OUT_RX_L_OP_DOC_STAGE2="(?mi)^\\s*<output>\\s*\$"
# OLLAMA_OUT_RX_R_OP_DOC_STAGE2="(?mi)^\\s*</output>\\s*\$"
# OLLAMA_THINK_RX_L_OP_EXPLAIN_STAGE1="(?mi)^\\s*<think>\\s*\$"
# OLLAMA_THINK_RX_R_OP_EXPLAIN_STAGE1="(?mi)^\\s*</think>\\s*\$"
# OLLAMA_OUT_RX_L_OP_EXPLAIN_STAGE1="(?mi)^\\s*<output>\\s*\$"
# OLLAMA_OUT_RX_R_OP_EXPLAIN_STAGE1="(?mi)^\\s*</output>\\s*\$"
# OLLAMA_THINK_RX_L_OP_EXPLAIN_STAGE2="(?mi)^\\s*<think>\\s*\$"
# OLLAMA_THINK_RX_R_OP_EXPLAIN_STAGE2="(?mi)^\\s*</think>\\s*\$"
# OLLAMA_OUT_RX_L_OP_EXPLAIN_STAGE2="(?mi)^\\s*<output>\\s*\$"
# OLLAMA_OUT_RX_R_OP_EXPLAIN_STAGE2="(?mi)^\\s*</output>\\s*\$"
# OLLAMA_THINK_RX_L="(?mi)^\\s*<think>\\s*\$"
# OLLAMA_THINK_RX_R="(?mi)^\\s*</think>\\s*\$"
# OLLAMA_OUT_RX_L="(?mi)^\\s*<output>\\s*\$"
# OLLAMA_OUT_RX_R="(?mi)^\\s*</output>\\s*\$"

# The remaining options are for fine-tuning the model, when using with smaller sub-15b models, their use may be cruical to make things work
# OLLAMA_TOP_K_OP_ANNOTATE="40"
# OLLAMA_TOP_K_OP_ANNOTATE_POST="40"
# OLLAMA_TOP_K_OP_IMPLEMENT_STAGE1="40"
# OLLAMA_TOP_K_OP_IMPLEMENT_STAGE2="40"
# OLLAMA_TOP_K_OP_IMPLEMENT_STAGE3="40"
# OLLAMA_TOP_K_OP_IMPLEMENT_STAGE4="40"
# OLLAMA_TOP_K_OP_DOC_STAGE1="40"
# OLLAMA_TOP_K_OP_DOC_STAGE2="40"
# OLLAMA_TOP_K_OP_EXPLAIN_STAGE1="40"
# OLLAMA_TOP_K_OP_EXPLAIN_STAGE2="40"
# OLLAMA_TOP_K="40"
# OLLAMA_TOP_P_OP_ANNOTATE="0.9"
# OLLAMA_TOP_P_OP_ANNOTATE_POST="0.9"
# OLLAMA_TOP_P_OP_IMPLEMENT_STAGE1="0.9"
# OLLAMA_TOP_P_OP_IMPLEMENT_STAGE2="0.9"
# OLLAMA_TOP_P_OP_IMPLEMENT_STAGE3="0.9"
# OLLAMA_TOP_P_OP_IMPLEMENT_STAGE4="0.9"
# OLLAMA_TOP_P_OP_DOC_STAGE1="0.9"
# OLLAMA_TOP_P_OP_DOC_STAGE2="0.9"
# OLLAMA_TOP_P_OP_EXPLAIN_STAGE1="0.9"
# OLLAMA_TOP_P_OP_EXPLAIN_STAGE2="0.9"
# OLLAMA_TOP_P="0.9"
# OLLAMA_SEED_OP_ANNOTATE="42"
# OLLAMA_SEED_OP_ANNOTATE_POST="42"
# OLLAMA_SEED_OP_IMPLEMENT_STAGE1="42"
# OLLAMA_SEED_OP_IMPLEMENT_STAGE2="42"
# OLLAMA_SEED_OP_IMPLEMENT_STAGE3="42"
# OLLAMA_SEED_OP_IMPLEMENT_STAGE4="42"
# OLLAMA_SEED_OP_DOC_STAGE1="42"
# OLLAMA_SEED_OP_DOC_STAGE2="42"
# OLLAMA_SEED_OP_EXPLAIN_STAGE1="42"
# OLLAMA_SEED_OP_EXPLAIN_STAGE2="42"
# OLLAMA_SEED="42"
# note: values slightly more than 1.0 seem to help against problems when LLM starts to generate repeated content indefinitely
OLLAMA_REPEAT_PENALTY_OP_ANNOTATE="1.1"
OLLAMA_REPEAT_PENALTY_OP_ANNOTATE_POST="1.1"
OLLAMA_REPEAT_PENALTY_OP_IMPLEMENT_STAGE1="1.0"
# OLLAMA_REPEAT_PENALTY_OP_IMPLEMENT_STAGE2="1.1"
OLLAMA_REPEAT_PENALTY_OP_IMPLEMENT_STAGE3="1.0"
# OLLAMA_REPEAT_PENALTY_OP_IMPLEMENT_STAGE4="1.0"
OLLAMA_REPEAT_PENALTY_OP_DOC_STAGE1="1.0"
# OLLAMA_REPEAT_PENALTY_OP_DOC_STAGE2="1.2"
OLLAMA_REPEAT_PENALTY_OP_EXPLAIN_STAGE1="1.0"
# OLLAMA_REPEAT_PENALTY_OP_EXPLAIN_STAGE2="1.1"
# OLLAMA_REPEAT_PENALTY="1.0"
# OLLAMA_FREQ_PENALTY_OP_ANNOTATE="1.0"
# OLLAMA_FREQ_PENALTY_OP_ANNOTATE_POST="1.0"
# OLLAMA_FREQ_PENALTY_OP_IMPLEMENT_STAGE1="1.0"
# OLLAMA_FREQ_PENALTY_OP_IMPLEMENT_STAGE2="1.0"
# OLLAMA_FREQ_PENALTY_OP_IMPLEMENT_STAGE3="1.0"
# OLLAMA_FREQ_PENALTY_OP_IMPLEMENT_STAGE4="1.0"
# OLLAMA_FREQ_PENALTY_OP_DOC_STAGE1="1.0"
# OLLAMA_FREQ_PENALTY_OP_DOC_STAGE2="1.0"
# OLLAMA_FREQ_PENALTY_OP_EXPLAIN_STAGE1="1.0"
# OLLAMA_FREQ_PENALTY_OP_EXPLAIN_STAGE2="1.0"
# OLLAMA_FREQ_PENALTY="1.0"
# OLLAMA_PRESENCE_PENALTY_OP_ANNOTATE="1.0"
# OLLAMA_PRESENCE_PENALTY_OP_ANNOTATE_POST="1.0"
# OLLAMA_PRESENCE_PENALTY_OP_IMPLEMENT_STAGE1="1.0"
# OLLAMA_PRESENCE_PENALTY_OP_IMPLEMENT_STAGE2="1.0"
# OLLAMA_PRESENCE_PENALTY_OP_IMPLEMENT_STAGE3="1.0"
# OLLAMA_PRESENCE_PENALTY_OP_IMPLEMENT_STAGE4="1.0"
# OLLAMA_PRESENCE_PENALTY_OP_DOC_STAGE1="1.0"
# OLLAMA_PRESENCE_PENALTY_OP_DOC_STAGE2="1.0"
# OLLAMA_PRESENCE_PENALTY_OP_EXPLAIN_STAGE1="1.0"
# OLLAMA_PRESENCE_PENALTY_OP_EXPLAIN_STAGE2="1.0"
# OLLAMA_PRESENCE_PENALTY="1.0"
