# Example .env config, version: development

# Configuration files should have ".env" extensions and it can be placed to the following locations:
# Project local config: <Project root>/.perpetual/*.env
# Global config. On Linux: ~/.config/Perpetual/*.env ; On Windows: <User profile dir>\AppData\Roaming\Perpetual\*.env
# Also, the parameters can be exported to the system environment before running the utility, then they will have priority over the parameters in the configuration files. The "*.env" files will be loaded in alphabetical order, with parameters in previously loaded files taking precedence.

# This particular example only contain provider selection for Perpetual operations and stages.
# You can offload tasks to different providers to balance between generation quality and costs.
# See other "*.env.example" file for LLM-providers specific parameters.

# For now 4 LLM providers are supported:
# "anthropic": which parameters started with ANTHROPIC_* prefix
# "openai": which parameters started with OPENAI_* prefix
# "ollama": which parameters started with OLLAMA_* prefix
# "generic": Generic OpenAI compatible provider, which parameters started with GENERIC_* prefix

# When dealing with files that cannot be read as proper UTF[8/16/32] encoded file, try using this fallback encoding as last resort.
# You can use encoding names supported by "golang.org/x/text/encoding/ianaindex" package
# FALLBACK_TEXT_ENCODING="windows-1252"

# Per-operation provider selection

# LLM_PROVIDER_OP_ANNOTATE="anthropic"
# LLM_PROVIDER_OP_ANNOTATE_POST="anthropic"
# LLM_PROVIDER_OP_EMBED="ollama"
# LLM_PROVIDER_OP_IMPLEMENT_STAGE1="anthropic"
# LLM_PROVIDER_OP_IMPLEMENT_STAGE2="anthropic"
# LLM_PROVIDER_OP_IMPLEMENT_STAGE3="anthropic"
# LLM_PROVIDER_OP_IMPLEMENT_STAGE4="anthropic"
# LLM_PROVIDER_OP_DOC_STAGE1="anthropic"
# LLM_PROVIDER_OP_DOC_STAGE2="anthropic"
# LLM_PROVIDER_OP_EXPLAIN_STAGE1="anthropic"
# LLM_PROVIDER_OP_EXPLAIN_STAGE2="anthropic"

# Default, fallback provider selection, will be used if parameters above are not set

LLM_PROVIDER="anthropic"
# LLM_PROVIDER="openai"
# LLM_PROVIDER="ollama"
# LLM_PROVIDER="generic"

# NOTE: you can also setup multiple profiles for supported LLM providers using following naming scheme: <PROVIDER><PROFILE NUMBER>_<OPTION>
# examples:

# LLM_PROVIDER="ollama1" # Will use parameters started with prefix OLLAMA1_*, like:
# OLLAMA1_BASE_URL=...
# OLLAMA1_MODEL=...

# LLM_PROVIDER="generic1"
# GENERIC1_BASE_URL=...
# GENERIC1_MODEL=...
