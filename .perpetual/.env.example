# Example .env config, version: development

# The following example contains all currently supported environment parameters.
# They are used to set up the connection to LLM and configure it for various operations. Your actual configuration may be significantly smaller.
# Some options are commented out, you can uncomment them to customize the behavior in special cases - it will take priority only for those specific cases.

# Configuration file should be named ".env" and it can be placed to the following locations:
# Project local config: <Project root>/.perpetual/.env
# Global config. On Linux: ~/.config/Perpetual/.env ; On Windows: <User profile dir>\AppData\Roaming\Perpetual\.env
# Also, the parameters can be exported to the system environment before running the utility, then they will have priority over the parameters in the configuration files.

# Provider selection for particular operations and stages
# You can offload tasks to different providers to balance between generation quality and costs.

# LLM_PROVIDER_OP_ANNOTATE="anthropic"
# LLM_PROVIDER_OP_ANNOTATE_POST="anthropic"
# LLM_PROVIDER_OP_EMBED="ollama"
# LLM_PROVIDER_OP_IMPLEMENT_STAGE1="anthropic"
# LLM_PROVIDER_OP_IMPLEMENT_STAGE2="anthropic"
# LLM_PROVIDER_OP_IMPLEMENT_STAGE3="anthropic"
# LLM_PROVIDER_OP_IMPLEMENT_STAGE4="anthropic"
# LLM_PROVIDER_OP_DOC_STAGE1="anthropic"
# LLM_PROVIDER_OP_DOC_STAGE2="anthropic"
# LLM_PROVIDER_OP_EXPLAIN_STAGE1="anthropic"
# LLM_PROVIDER_OP_EXPLAIN_STAGE2="anthropic"

# Default, fallback provider selection, will be used if options above are not used

LLM_PROVIDER="anthropic"
# LLM_PROVIDER="openai"
# LLM_PROVIDER="ollama"
# LLM_PROVIDER="generic"

# NOTE: you can also setup multiple profiles for supported LLM providers using following naming scheme: <PROVIDER><PROFILE NUMBER>_<OPTION>
# examples:

# LLM_PROVIDER="ollama1"
# OLLAMA1_BASE_URL=...
# OLLAMA1_MODEL=...

# LLM_PROVIDER="generic1"
# GENERIC1_BASE_URL=...
# GENERIC1_MODEL=...


# Options for Anthropic provider. Below are sane defaults for Anthropic provider (as of Jan 2025)
# NOTE: Anthropic provider has no embedding models (as for Apr 2025)

ANTHROPIC_API_KEY="<your api key goes here>"
ANTHROPIC_BASE_URL="https://api.anthropic.com/v1"
ANTHROPIC_MODEL_OP_ANNOTATE="claude-3-haiku-20240307"
ANTHROPIC_MODEL_OP_ANNOTATE_POST="claude-3-haiku-20240307" # used to process multiple response-variants if any
# ANTHROPIC_MODEL_OP_IMPLEMENT_STAGE1="claude-3-7-sonnet-latest"
# ANTHROPIC_MODEL_OP_IMPLEMENT_STAGE2="claude-3-7-sonnet-latest"
# ANTHROPIC_MODEL_OP_IMPLEMENT_STAGE3="claude-3-7-sonnet-latest"
# ANTHROPIC_MODEL_OP_IMPLEMENT_STAGE4="claude-3-7-sonnet-latest"
# ANTHROPIC_MODEL_OP_DOC_STAGE1="claude-3-7-sonnet-latest"
ANTHROPIC_MODEL_OP_DOC_STAGE2="claude-3-7-sonnet-latest"
# ANTHROPIC_MODEL_OP_EXPLAIN_STAGE1="claude-3-7-sonnet-latest"
ANTHROPIC_MODEL_OP_EXPLAIN_STAGE2="claude-3-7-sonnet-latest"
ANTHROPIC_MODEL="claude-3-7-sonnet-latest"
ANTHROPIC_VARIANT_COUNT_OP_ANNOTATE="1" # how much annotate-response variants to generate
ANTHROPIC_VARIANT_SELECTION_OP_ANNOTATE="short" # how to select final variant: short, long, combine, best
ANTHROPIC_VARIANT_COUNT="1" # will be used as fallback
ANTHROPIC_VARIANT_SELECTION="short" # will be used as fallback

# Switch to use structured JSON output format for supported operations, supported values: plain, json. Default: plain
# The "plain" method seems to work better here, since it uses XML-style tags, for which anthropic models were initially better trained than for JSON.
# ANTHROPIC_FORMAT_OP_IMPLEMENT_STAGE1="json"
# ANTHROPIC_FORMAT_OP_IMPLEMENT_STAGE3="json"
# ANTHROPIC_FORMAT_OP_DOC_STAGE1="json"
# ANTHROPIC_FORMAT_OP_EXPLAIN_STAGE1="json"

ANTHROPIC_MAX_TOKENS_OP_ANNOTATE="768"
ANTHROPIC_MAX_TOKENS_OP_ANNOTATE_POST="768"
ANTHROPIC_MAX_TOKENS_OP_IMPLEMENT_STAGE1="512" # file-list for review, long list is probably an error
ANTHROPIC_MAX_TOKENS_OP_IMPLEMENT_STAGE2="1536" # work plan also should not be too big
ANTHROPIC_MAX_TOKENS_OP_IMPLEMENT_STAGE3="512" # file-list for processing, long list is probably an error
ANTHROPIC_MAX_TOKENS_OP_IMPLEMENT_STAGE4="8192" # generated code output limit should be as big as possible
ANTHROPIC_MAX_TOKENS_OP_DOC_STAGE1="768" # file-list for review, long list is probably an error
ANTHROPIC_MAX_TOKENS_OP_DOC_STAGE2="32768" # generated document output limit should be big
ANTHROPIC_MAX_TOKENS_OP_EXPLAIN_STAGE1="512" # file-list for review
ANTHROPIC_MAX_TOKENS_OP_EXPLAIN_STAGE2="32768" # generated answer output limit
ANTHROPIC_MAX_TOKENS="4096" # default limit
ANTHROPIC_MAX_TOKENS_SEGMENTS="3"

# Extended thinking. Should work with newer models. 1024 is a minimum for claude 3.7 sonnet.
# May be incompatible with some other parameters (temperature)
# If set to 0 - explicitly disable extra thinking in api call,
# if > 0 enable thinking block in api call and set budget_tokens,
# If unset - do not alter api call and response in any way
# ANTHROPIC_THINK_TOKENS_OP_ANNOTATE="0"
# ANTHROPIC_THINK_TOKENS_OP_ANNOTATE_POST="0"
# ANTHROPIC_THINK_TOKENS_OP_IMPLEMENT_STAGE1="0" # file list
# ANTHROPIC_THINK_TOKENS_OP_IMPLEMENT_STAGE2="2048" # work plan
# ANTHROPIC_THINK_TOKENS_OP_IMPLEMENT_STAGE3="0" # file list
# ANTHROPIC_THINK_TOKENS_OP_IMPLEMENT_STAGE4="1024" # code implementation
# ANTHROPIC_THINK_TOKENS_OP_DOC_STAGE1="0" # file list
ANTHROPIC_THINK_TOKENS_OP_DOC_STAGE2="4096" # document process
# ANTHROPIC_THINK_TOKENS_OP_EXPLAIN_STAGE1="0" # file list
ANTHROPIC_THINK_TOKENS_OP_EXPLAIN_STAGE2="4096" # answer generation
ANTHROPIC_THINK_TOKENS="0"

ANTHROPIC_ON_FAIL_RETRIES_OP_ANNOTATE="1"
# ANTHROPIC_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE1="3"
# ANTHROPIC_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE2="3"
# ANTHROPIC_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE3="3"
# ANTHROPIC_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE4="3"
# ANTHROPIC_ON_FAIL_RETRIES_OP_DOC_STAGE1="3"
# ANTHROPIC_ON_FAIL_RETRIES_OP_DOC_STAGE2="3"
# ANTHROPIC_ON_FAIL_RETRIES_OP_EXPLAIN_STAGE1="3"
# ANTHROPIC_ON_FAIL_RETRIES_OP_EXPLAIN_STAGE2="3"
ANTHROPIC_ON_FAIL_RETRIES="3"
# ANTHROPIC_TEMPERATURE_OP_ANNOTATE="0.5"
# ANTHROPIC_TEMPERATURE_OP_ANNOTATE_POST="0.5"
ANTHROPIC_TEMPERATURE_OP_IMPLEMENT_STAGE1="0.2" # less creative for file-list output
# ANTHROPIC_TEMPERATURE_OP_IMPLEMENT_STAGE2="0.5"
ANTHROPIC_TEMPERATURE_OP_IMPLEMENT_STAGE3="0.2" # less creative for file-list output
# ANTHROPIC_TEMPERATURE_OP_IMPLEMENT_STAGE4="0.5"
ANTHROPIC_TEMPERATURE_OP_DOC_STAGE1="0.2" # less creative for file-list output
ANTHROPIC_TEMPERATURE_OP_DOC_STAGE2="1" # value 1 for needed thinking model
ANTHROPIC_TEMPERATURE_OP_EXPLAIN_STAGE1="0.2" # less creative for file-list output
ANTHROPIC_TEMPERATURE_OP_EXPLAIN_STAGE2="1" # value 1 for needed thinking model
ANTHROPIC_TEMPERATURE="0.5"

# Advanced options that currently supported with Anthropic. You mostly not need to use them
# ANTHROPIC_TOP_K_OP_ANNOTATE="40"
# ANTHROPIC_TOP_K_OP_ANNOTATE_POST="40"
# ANTHROPIC_TOP_K_OP_IMPLEMENT_STAGE1="40"
# ANTHROPIC_TOP_K_OP_IMPLEMENT_STAGE2="40"
# ANTHROPIC_TOP_K_OP_IMPLEMENT_STAGE3="40"
# ANTHROPIC_TOP_K_OP_IMPLEMENT_STAGE4="40"
# ANTHROPIC_TOP_K_OP_DOC_STAGE1="40"
# ANTHROPIC_TOP_K_OP_DOC_STAGE2="40"
# ANTHROPIC_TOP_K_OP_EXPLAIN_STAGE1="40"
# ANTHROPIC_TOP_K_OP_EXPLAIN_STAGE2="40"
# ANTHROPIC_TOP_K="40"
# ANTHROPIC_TOP_P_OP_ANNOTATE="0.9"
# ANTHROPIC_TOP_P_OP_ANNOTATE_POST="0.9"
# ANTHROPIC_TOP_P_OP_IMPLEMENT_STAGE1="0.9"
# ANTHROPIC_TOP_P_OP_IMPLEMENT_STAGE2="0.9"
# ANTHROPIC_TOP_P_OP_IMPLEMENT_STAGE3="0.9"
# ANTHROPIC_TOP_P_OP_IMPLEMENT_STAGE4="0.9"
# ANTHROPIC_TOP_P_OP_DOC_STAGE1="0.9"
# ANTHROPIC_TOP_P_OP_DOC_STAGE2="0.9"
# ANTHROPIC_TOP_P_OP_EXPLAIN_STAGE1="0.9"
# ANTHROPIC_TOP_P_OP_EXPLAIN_STAGE2="0.9"
# ANTHROPIC_TOP_P="0.9"


# Options for OpenAI provider. Below are sane defaults for OpenAI provider (as of Jan 2025)

OPENAI_API_KEY="<your api key goes here>"
OPENAI_BASE_URL="https://api.openai.com/v1"
OPENAI_MODEL_OP_ANNOTATE="gpt-4o-mini"
OPENAI_MODEL_OP_ANNOTATE_POST="gpt-4o-mini" # used to process multiple response-variants if any
OPENAI_MODEL_OP_EMBED="text-embedding-3-small"
# OPENAI_MODEL_OP_IMPLEMENT_STAGE1="gpt-4o"
# OPENAI_MODEL_OP_IMPLEMENT_STAGE2="gpt-4o"
# OPENAI_MODEL_OP_IMPLEMENT_STAGE3="gpt-4o"
# OPENAI_MODEL_OP_IMPLEMENT_STAGE4="gpt-4o"
OPENAI_MODEL_OP_DOC_STAGE1="gpt-4o"
# OPENAI_MODEL_OP_DOC_STAGE2="o1-mini-2024-09-12" # good for generating initial document structure from your draft
# OPENAI_MODEL_OP_DOC_STAGE2="gpt-4o"
OPENAI_MODEL_OP_DOC_STAGE2="gpt-4.5-preview"
# OPENAI_MODEL_OP_EXPLAIN_STAGE1="gpt-4o"
# OPENAI_MODEL_OP_EXPLAIN_STAGE2="gpt-4o"
# OPENAI_MODEL_OP_EXPLAIN_STAGE2="o1-mini-2024-09-12"
# OPENAI_MODEL_OP_EXPLAIN_STAGE2="gpt-4.5-preview"
OPENAI_MODEL="gpt-4o"
OPENAI_VARIANT_COUNT_OP_ANNOTATE="1" # how much annotate-response variants to generate
OPENAI_VARIANT_SELECTION_OP_ANNOTATE="short" # how to select final variant: short, long, combine, best
OPENAI_VARIANT_COUNT="1" # will be used as fallback
OPENAI_VARIANT_SELECTION="short" # will be used as fallback

# Text chunk/sequence size in characters (not tokens), used when generating embeddings.
# Values too small or too large may lead to less effective search.
# OPENAI_EMBED_CHUNK_SIZE="2048"
# OPENAI_EMBED_CHUNK_OVERLAP="256"

# Set dimension count of generated vectors, supported for text-embedding-3 models, usually not need to change anything here.
# OPENAI_EMBED_DIMENSIONS="1536" # not set by default

# Cosine score threshold value to consider search vector simiar to target, usually not need to change anything here.
# Model dependent, may be less than 0 but seem not for OpenAI, (score < 0 usually means that the vectors are semantically opposite)
# OPENAI_EMBED_SCORE_THRESHOLD="0.0"

# Switch to use structured JSON output format for supported operations
# Supported values: plain, json. Default: plain
# OPENAI_FORMAT_OP_IMPLEMENT_STAGE1="json"
# OPENAI_FORMAT_OP_IMPLEMENT_STAGE3="json"
# OPENAI_FORMAT_OP_DOC_STAGE1="json"
# OPENAI_FORMAT_OP_EXPLAIN_STAGE1="json"

OPENAI_MAX_TOKENS_OP_ANNOTATE="768"
OPENAI_MAX_TOKENS_OP_ANNOTATE_POST="768"
OPENAI_MAX_TOKENS_OP_IMPLEMENT_STAGE1="512" # file-list for review, long list is probably an error
OPENAI_MAX_TOKENS_OP_IMPLEMENT_STAGE2="1536" # work plan also should not be too big
OPENAI_MAX_TOKENS_OP_IMPLEMENT_STAGE3="512" # file-list for processing, long list is probably an error
OPENAI_MAX_TOKENS_OP_IMPLEMENT_STAGE4="16384" # generated code output limit should be as big as possible
OPENAI_MAX_TOKENS_OP_DOC_STAGE1="768" # file-list for review, long list is probably an error
OPENAI_MAX_TOKENS_OP_DOC_STAGE2="16384" # generated document output limit should be as big as possible
OPENAI_MAX_TOKENS_OP_EXPLAIN_STAGE1="512" # file-list for review
OPENAI_MAX_TOKENS_OP_EXPLAIN_STAGE2="8192" # generated answer output limit
OPENAI_MAX_TOKENS="4096" # default limit
OPENAI_MAX_TOKENS_SEGMENTS="3"
OPENAI_ON_FAIL_RETRIES_OP_ANNOTATE="1"
# OPENAI_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE1="3"
# OPENAI_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE2="3"
# OPENAI_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE3="3"
# OPENAI_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE4="3"
# OPENAI_ON_FAIL_RETRIES_OP_DOC_STAGE1="3"
# OPENAI_ON_FAIL_RETRIES_OP_DOC_STAGE2="3"
# OPENAI_ON_FAIL_RETRIES_OP_EXPLAIN_STAGE1="3"
# OPENAI_ON_FAIL_RETRIES_OP_EXPLAIN_STAGE2="3"
OPENAI_ON_FAIL_RETRIES="3"
# OPENAI_TEMPERATURE_OP_ANNOTATE="0.5"
# OPENAI_TEMPERATURE_OP_ANNOTATE_POST="0.5"
OPENAI_TEMPERATURE_OP_IMPLEMENT_STAGE1="0.2" # less creative for file-list output
# OPENAI_TEMPERATURE_OP_IMPLEMENT_STAGE2="0.5"
OPENAI_TEMPERATURE_OP_IMPLEMENT_STAGE3="0.2" # less creative for file-list output
# OPENAI_TEMPERATURE_OP_IMPLEMENT_STAGE4="0.5"
OPENAI_TEMPERATURE_OP_DOC_STAGE1="0.2" # less creative for file-list output
OPENAI_TEMPERATURE_OP_DOC_STAGE2="0.7" # more creative when writing documentation
OPENAI_TEMPERATURE_OP_EXPLAIN_STAGE1="0.2" # less creative for file-list output
OPENAI_TEMPERATURE_OP_EXPLAIN_STAGE2="0.7"
OPENAI_TEMPERATURE="0.5"

# Advanced options for finetuning. Generally you do not need them.
# OPENAI_TOP_P_OP_ANNOTATE="0.9"
# OPENAI_TOP_P_OP_ANNOTATE_POST="0.9"
# OPENAI_TOP_P_OP_IMPLEMENT_STAGE1="0.9"
# OPENAI_TOP_P_OP_IMPLEMENT_STAGE2="0.9"
# OPENAI_TOP_P_OP_IMPLEMENT_STAGE3="0.9"
# OPENAI_TOP_P_OP_IMPLEMENT_STAGE4="0.9"
# OPENAI_TOP_P_OP_DOC_STAGE1="0.9"
# OPENAI_TOP_P_OP_DOC_STAGE2="0.9"
# OPENAI_TOP_P_OP_EXPLAIN_STAGE1="0.9"
# OPENAI_TOP_P_OP_EXPLAIN_STAGE2="0.9"
# OPENAI_TOP_P="0.9"
# OPENAI_SEED_OP_ANNOTATE="42"
# OPENAI_SEED_OP_ANNOTATE_POST="42"
# OPENAI_SEED_OP_IMPLEMENT_STAGE1="42"
# OPENAI_SEED_OP_IMPLEMENT_STAGE2="42"
# OPENAI_SEED_OP_IMPLEMENT_STAGE3="42"
# OPENAI_SEED_OP_IMPLEMENT_STAGE4="42"
# OPENAI_SEED_OP_DOC_STAGE1="42"
# OPENAI_SEED_OP_DOC_STAGE2="42"
# OPENAI_SEED_OP_EXPLAIN_STAGE1="42"
# OPENAI_SEED_OP_EXPLAIN_STAGE2="42"
# OPENAI_SEED="42"
# OPENAI_REASONING_EFFORT_OP_ANNOTATE="medium" # will work only for some reasoning models like full o1 (not o1-preview or o1-mini), values: low, medium, high
# OPENAI_REASONING_EFFORT_OP_ANNOTATE_POST="medium"
# OPENAI_REASONING_EFFORT_OP_IMPLEMENT_STAGE1="medium"
# OPENAI_REASONING_EFFORT_OP_IMPLEMENT_STAGE2="medium"
# OPENAI_REASONING_EFFORT_OP_IMPLEMENT_STAGE3="medium"
# OPENAI_REASONING_EFFORT_OP_IMPLEMENT_STAGE4="medium"
# OPENAI_REASONING_EFFORT_OP_DOC_STAGE1="medium"
# OPENAI_REASONING_EFFORT_OP_DOC_STAGE2="medium"
# OPENAI_REASONING_EFFORT_OP_EXPLAIN_STAGE1="medium"
# OPENAI_REASONING_EFFORT_OP_EXPLAIN_STAGE2="medium"
# OPENAI_REASONING_EFFORT="medium"
# OPENAI_FREQ_PENALTY_OP_ANNOTATE="1.0"
# OPENAI_FREQ_PENALTY_OP_ANNOTATE_POST="1.0"
# OPENAI_FREQ_PENALTY_OP_IMPLEMENT_STAGE1="1.0"
# OPENAI_FREQ_PENALTY_OP_IMPLEMENT_STAGE2="1.0"
# OPENAI_FREQ_PENALTY_OP_IMPLEMENT_STAGE3="1.0"
# OPENAI_FREQ_PENALTY_OP_IMPLEMENT_STAGE4="1.0"
# OPENAI_FREQ_PENALTY_OP_DOC_STAGE1="1.0"
# OPENAI_FREQ_PENALTY_OP_DOC_STAGE2="1.0"
# OPENAI_FREQ_PENALTY_OP_EXPLAIN_STAGE1="1.0"
# OPENAI_FREQ_PENALTY_OP_EXPLAIN_STAGE2="1.0"
# OPENAI_FREQ_PENALTY="1.0"
# OPENAI_PRESENCE_PENALTY_OP_ANNOTATE="1.0"
# OPENAI_PRESENCE_PENALTY_OP_ANNOTATE_POST="1.0"
# OPENAI_PRESENCE_PENALTY_OP_IMPLEMENT_STAGE1="1.0"
# OPENAI_PRESENCE_PENALTY_OP_IMPLEMENT_STAGE2="1.0"
# OPENAI_PRESENCE_PENALTY_OP_IMPLEMENT_STAGE3="1.0"
# OPENAI_PRESENCE_PENALTY_OP_IMPLEMENT_STAGE4="1.0"
# OPENAI_PRESENCE_PENALTY_OP_DOC_STAGE1="1.0"
# OPENAI_PRESENCE_PENALTY_OP_DOC_STAGE2="1.0"
# OPENAI_PRESENCE_PENALTY_OP_EXPLAIN_STAGE1="1.0"
# OPENAI_PRESENCE_PENALTY_OP_EXPLAIN_STAGE2="1.0"
# OPENAI_PRESENCE_PENALTY="1.0"


# Options for Ollama integration, running locally.
# When using a large enough model it can produce good results for some operations. See docs/ollama.md for more info

# OLLAMA_BASE_URL="http://127.0.0.1:11434"

# Optional authentication, for use with external https proxy
# OLLAMA_AUTH_TYPE="Bearer" # Type of the authentication used, "Bearer" - api key or token (default), "Basic" - web auth with login and password.
# OLLAMA_AUTH="<your api-key or token goes here>" # When using bearer auth type, put your api key or auth token here
# OLLAMA_AUTH="<login>:<password>" # Web auth requres login and password separated by a colon

OLLAMA_MODEL_OP_ANNOTATE="qwen2.5-coder:7b-instruct-q5_K_M"
OLLAMA_MODEL_OP_ANNOTATE_POST="qwen2.5-coder:7b-instruct-q5_K_M" # used to process multiple response-variants if any
OLLAMA_MODEL_OP_EMBED="snowflake-arctic-embed2" # embedding model currently do not have any other settings, you should use modelfile to customize it
# OLLAMA_MODEL_OP_IMPLEMENT_STAGE1="qwen2.5-coder:14b-instruct-q4_K_M"
# OLLAMA_MODEL_OP_IMPLEMENT_STAGE2="qwen2.5-coder:14b-instruct-q4_K_M"
# OLLAMA_MODEL_OP_IMPLEMENT_STAGE3="qwen2.5-coder:14b-instruct-q4_K_M"
# OLLAMA_MODEL_OP_IMPLEMENT_STAGE4="qwen2.5-coder:14b-instruct-q4_K_M"
# OLLAMA_MODEL_OP_DOC_STAGE1="llama3.1:70b-instruct-q4_K_S"
# OLLAMA_MODEL_OP_DOC_STAGE2="llama3.1:70b-instruct-q4_K_S"
# OLLAMA_MODEL_OP_EXPLAIN_STAGE1="llama3.1:70b-instruct-q4_K_S"
# OLLAMA_MODEL_OP_EXPLAIN_STAGE2="llama3.1:70b-instruct-q4_K_S"
OLLAMA_MODEL="qwen2.5-coder:14b-instruct-q4_K_M"
OLLAMA_VARIANT_COUNT_OP_ANNOTATE="1" # how much annotate-response variants to generate
OLLAMA_VARIANT_SELECTION_OP_ANNOTATE="short" # how to select final variant: short, long, combine, best
OLLAMA_VARIANT_COUNT="1" # will be used as fallback
OLLAMA_VARIANT_SELECTION="short" # will be used as fallback

# Text chunk/sequence size in characters (not tokens), used when generating embeddings.
# Values too small or too large may lead to less effective search. Highly model-dependent.
# OLLAMA_EMBED_CHUNK_SIZE="2048"
# OLLAMA_EMBED_CHUNK_OVERLAP="256"

# Cosine score threshold value to consider search vector simiar to target, usually not need to change anything here.
# Model dependent, may be less than 0 for some models, (score < 0 usually means that the vectors are semantically opposite)
# OLLAMA_EMBED_SCORE_THRESHOLD="0.0"

# LLM query text prefix used when generating embeddings for project files. Model dependent. Unset by default.
# OLLAMA_EMBED_DOC_PREFIX="Process following document:\n"
# LLM query text prefix used when generating embeddings for search queries. Model dependent. Unset by default.
# OLLAMA_EMBED_SEARCH_PREFIX="Process following search query:\n"

# examples for some models:
# nomic-embed-text-v1.5
# OLLAMA_EMBED_DOC_PREFIX="search_document: \n"
# OLLAMA_EMBED_SEARCH_PREFIX="search_query: \n"
# mxbai-embed-large-v1
# OLLAMA_EMBED_DOC_PREFIX=""
# OLLAMA_EMBED_SEARCH_PREFIX="Represent this sentence for searching relevant passages: \n"

# Context overflow detection and management options: multiplier to increase context size, upper limit, and estimation multiplier.
# Used to detect overflow and increase context size on errors if needed. Context size will reset back to initial value when starting new operation stage.
# These params may be removed in future when Ollama implement API calls for tokenizer or prompt size statistics
OLLAMA_CONTEXT_MULT="1.75"
OLLAMA_CONTEXT_SIZE_LIMIT="65536" # reasonable limit for typical desktop systems
OLLAMA_CONTEXT_ESTIMATE_MULT="0.3"

# Context window sizes for different operations, if set too low, it will be extended automatically when context overflow detected
# If not set - use default for ollama model, and also disable context overflow detection above
OLLAMA_CONTEXT_SIZE_OP_ANNOTATE="8192"
OLLAMA_CONTEXT_SIZE_OP_ANNOTATE_POST="8192"
OLLAMA_CONTEXT_SIZE_OP_IMPLEMENT_STAGE1="24576"
OLLAMA_CONTEXT_SIZE_OP_IMPLEMENT_STAGE2="24576"
OLLAMA_CONTEXT_SIZE_OP_IMPLEMENT_STAGE3="24576"
OLLAMA_CONTEXT_SIZE_OP_IMPLEMENT_STAGE4="24576"
OLLAMA_CONTEXT_SIZE_OP_DOC_STAGE1="49152"
OLLAMA_CONTEXT_SIZE_OP_DOC_STAGE2="49152"
OLLAMA_CONTEXT_SIZE_OP_EXPLAIN_STAGE1="24576"
OLLAMA_CONTEXT_SIZE_OP_EXPLAIN_STAGE2="24576"
OLLAMA_CONTEXT_SIZE="24576"

# Switch to use structured JSON output format for some operations, may work better with some models (or not work at all)
# Supported values: plain, json. Default: plain
OLLAMA_FORMAT_OP_IMPLEMENT_STAGE1="json"
OLLAMA_FORMAT_OP_IMPLEMENT_STAGE3="json"
OLLAMA_FORMAT_OP_DOC_STAGE1="json"
OLLAMA_FORMAT_OP_EXPLAIN_STAGE1="json"

OLLAMA_MAX_TOKENS_OP_ANNOTATE="768"
OLLAMA_MAX_TOKENS_OP_ANNOTATE_POST="768"
OLLAMA_MAX_TOKENS_OP_IMPLEMENT_STAGE1="512" # file-list for review, long list is probably an error
OLLAMA_MAX_TOKENS_OP_IMPLEMENT_STAGE2="1536" # work plan also should not be too big
OLLAMA_MAX_TOKENS_OP_IMPLEMENT_STAGE3="512" # file-list for processing, long list is probably an error
OLLAMA_MAX_TOKENS_OP_IMPLEMENT_STAGE4="4096" # generated code output limit should be as big as possible
OLLAMA_MAX_TOKENS_OP_DOC_STAGE1="768" # file-list for review, long list is probably an error
OLLAMA_MAX_TOKENS_OP_DOC_STAGE2="4096" # generated document output limit should be as big as possible
OLLAMA_MAX_TOKENS_OP_EXPLAIN_STAGE1="512" # file-list for review
OLLAMA_MAX_TOKENS_OP_EXPLAIN_STAGE2="8192" # generated answer output limit
OLLAMA_MAX_TOKENS="4096"
OLLAMA_MAX_TOKENS_SEGMENTS="3"
OLLAMA_ON_FAIL_RETRIES_OP_ANNOTATE="5" # this number include errors caused by context overflow
# OLLAMA_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE1="3"
# OLLAMA_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE2="3"
# OLLAMA_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE3="3"
# OLLAMA_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE4="3"
# OLLAMA_ON_FAIL_RETRIES_OP_DOC_STAGE1="3"
# OLLAMA_ON_FAIL_RETRIES_OP_DOC_STAGE2="3"
# OLLAMA_ON_FAIL_RETRIES_OP_EXPLAIN_STAGE1="3"
# OLLAMA_ON_FAIL_RETRIES_OP_EXPLAIN_STAGE2="3"
OLLAMA_ON_FAIL_RETRIES="3"
# note: temperature highly depends on model, 0 produces mostly deterministic results
OLLAMA_TEMPERATURE_OP_ANNOTATE="0.5"
OLLAMA_TEMPERATURE_OP_ANNOTATE_POST="0"
OLLAMA_TEMPERATURE_OP_IMPLEMENT_STAGE1="0.2" # less creative for file-list output
# OLLAMA_TEMPERATURE_OP_IMPLEMENT_STAGE2="0.5"
OLLAMA_TEMPERATURE_OP_IMPLEMENT_STAGE3="0.2" # less creative for file-list output
# OLLAMA_TEMPERATURE_OP_IMPLEMENT_STAGE4="0.5"
OLLAMA_TEMPERATURE_OP_DOC_STAGE1="0.2" # less creative for file-list output
OLLAMA_TEMPERATURE_OP_DOC_STAGE2="0.7"
OLLAMA_TEMPERATURE_OP_EXPLAIN_STAGE1="0.2" # less creative for file-list output
OLLAMA_TEMPERATURE_OP_EXPLAIN_STAGE2="0.7"
OLLAMA_TEMPERATURE="0.5"

# System prompt role for model, can be configured per operation. Useful if model not supporting system prompt
# Valid values: system, user. default: system.
# When using "user" role, system prompt will be converted to user-query + ai-acknowledge message sequence
# OLLAMA_SYSPROMPT_ROLE_OP_ANNOTATE="system"
# OLLAMA_SYSPROMPT_ROLE_OP_ANNOTATE_POST="system"
# OLLAMA_SYSPROMPT_ROLE_OP_IMPLEMENT_STAGE1="system"
# OLLAMA_SYSPROMPT_ROLE_OP_IMPLEMENT_STAGE2="system"
# OLLAMA_SYSPROMPT_ROLE_OP_IMPLEMENT_STAGE3="system"
# OLLAMA_SYSPROMPT_ROLE_OP_IMPLEMENT_STAGE4="system"
# OLLAMA_SYSPROMPT_ROLE_OP_DOC_STAGE1="system"
# OLLAMA_SYSPROMPT_ROLE_OP_DOC_STAGE2="system"
OLLAMA_SYSPROMPT_ROLE_OP_EXPLAIN_STAGE1="system"
OLLAMA_SYSPROMPT_ROLE_OP_EXPLAIN_STAGE2="system"
# OLLAMA_SYSPROMPT_ROLE="system"

# Optional regexps for filtering out responses from reasoning models, like deepseek r1
# THINK-regexps will be used to remove reasoning part from response L - is for opening tag, R - is for closing tag
# OUT-regexps will be used to extract output part from response after it was filered out with THINK-regexps
# OLLAMA_THINK_RX_L_OP_ANNOTATE="(?mi)^\\s*<think>\\s*\$"
# OLLAMA_THINK_RX_R_OP_ANNOTATE="(?mi)^\\s*</think>\\s*\$"
# OLLAMA_OUT_RX_L_OP_ANNOTATE="(?mi)^\\s*<output>\\s*\$"
# OLLAMA_OUT_RX_R_OP_ANNOTATE="(?mi)^\\s*</output>\\s*\$"
# OLLAMA_THINK_RX_L_OP_ANNOTATE_POST="(?mi)^\\s*<think>\\s*\$"
# OLLAMA_THINK_RX_R_OP_ANNOTATE_POST="(?mi)^\\s*</think>\\s*\$"
# OLLAMA_OUT_RX_L_OP_ANNOTATE_POST="(?mi)^\\s*<output>\\s*\$"
# OLLAMA_OUT_RX_R_OP_ANNOTATE_POST="(?mi)^\\s*</output>\\s*\$"
# OLLAMA_THINK_RX_L_OP_IMPLEMENT_STAGE1="(?mi)^\\s*<think>\\s*\$"
# OLLAMA_THINK_RX_R_OP_IMPLEMENT_STAGE1="(?mi)^\\s*</think>\\s*\$"
# OLLAMA_OUT_RX_L_OP_IMPLEMENT_STAGE1="(?mi)^\\s*<output>\\s*\$"
# OLLAMA_OUT_RX_R_OP_IMPLEMENT_STAGE1="(?mi)^\\s*</output>\\s*\$"
# OLLAMA_THINK_RX_L_OP_IMPLEMENT_STAGE2="(?mi)^\\s*<think>\\s*\$"
# OLLAMA_THINK_RX_R_OP_IMPLEMENT_STAGE2="(?mi)^\\s*</think>\\s*\$"
# OLLAMA_OUT_RX_L_OP_IMPLEMENT_STAGE2="(?mi)^\\s*<output>\\s*\$"
# OLLAMA_OUT_RX_R_OP_IMPLEMENT_STAGE2="(?mi)^\\s*</output>\\s*\$"
# OLLAMA_THINK_RX_L_OP_IMPLEMENT_STAGE3="(?mi)^\\s*<think>\\s*\$"
# OLLAMA_THINK_RX_R_OP_IMPLEMENT_STAGE3="(?mi)^\\s*</think>\\s*\$"
# OLLAMA_OUT_RX_L_OP_IMPLEMENT_STAGE3="(?mi)^\\s*<output>\\s*\$"
# OLLAMA_OUT_RX_R_OP_IMPLEMENT_STAGE3="(?mi)^\\s*</output>\\s*\$"
# OLLAMA_THINK_RX_L_OP_IMPLEMENT_STAGE4="(?mi)^\\s*<think>\\s*\$"
# OLLAMA_THINK_RX_R_OP_IMPLEMENT_STAGE4="(?mi)^\\s*</think>\\s*\$"
# OLLAMA_OUT_RX_L_OP_IMPLEMENT_STAGE4="(?mi)^\\s*<output>\\s*\$"
# OLLAMA_OUT_RX_R_OP_IMPLEMENT_STAGE4="(?mi)^\\s*</output>\\s*\$"
# OLLAMA_THINK_RX_L_OP_DOC_STAGE1="(?mi)^\\s*<think>\\s*\$"
# OLLAMA_THINK_RX_R_OP_DOC_STAGE1="(?mi)^\\s*</think>\\s*\$"
# OLLAMA_OUT_RX_L_OP_DOC_STAGE1="(?mi)^\\s*<output>\\s*\$"
# OLLAMA_OUT_RX_R_OP_DOC_STAGE1="(?mi)^\\s*</output>\\s*\$"
# OLLAMA_THINK_RX_L_OP_DOC_STAGE2="(?mi)^\\s*<think>\\s*\$"
# OLLAMA_THINK_RX_R_OP_DOC_STAGE2="(?mi)^\\s*</think>\\s*\$"
# OLLAMA_OUT_RX_L_OP_DOC_STAGE2="(?mi)^\\s*<output>\\s*\$"
# OLLAMA_OUT_RX_R_OP_DOC_STAGE2="(?mi)^\\s*</output>\\s*\$"
# OLLAMA_THINK_RX_L_OP_EXPLAIN_STAGE1="(?mi)^\\s*<think>\\s*\$"
# OLLAMA_THINK_RX_R_OP_EXPLAIN_STAGE1="(?mi)^\\s*</think>\\s*\$"
# OLLAMA_OUT_RX_L_OP_EXPLAIN_STAGE1="(?mi)^\\s*<output>\\s*\$"
# OLLAMA_OUT_RX_R_OP_EXPLAIN_STAGE1="(?mi)^\\s*</output>\\s*\$"
# OLLAMA_THINK_RX_L_OP_EXPLAIN_STAGE2="(?mi)^\\s*<think>\\s*\$"
# OLLAMA_THINK_RX_R_OP_EXPLAIN_STAGE2="(?mi)^\\s*</think>\\s*\$"
# OLLAMA_OUT_RX_L_OP_EXPLAIN_STAGE2="(?mi)^\\s*<output>\\s*\$"
# OLLAMA_OUT_RX_R_OP_EXPLAIN_STAGE2="(?mi)^\\s*</output>\\s*\$"
# OLLAMA_THINK_RX_L="(?mi)^\\s*<think>\\s*\$"
# OLLAMA_THINK_RX_R="(?mi)^\\s*</think>\\s*\$"
# OLLAMA_OUT_RX_L="(?mi)^\\s*<output>\\s*\$"
# OLLAMA_OUT_RX_R="(?mi)^\\s*</output>\\s*\$"

# The remaining options are for fine-tuning the model, when using with smaller sub-15b models, their use may be cruical to make things work
# OLLAMA_TOP_K_OP_ANNOTATE="40"
# OLLAMA_TOP_K_OP_ANNOTATE_POST="40"
# OLLAMA_TOP_K_OP_IMPLEMENT_STAGE1="40"
# OLLAMA_TOP_K_OP_IMPLEMENT_STAGE2="40"
# OLLAMA_TOP_K_OP_IMPLEMENT_STAGE3="40"
# OLLAMA_TOP_K_OP_IMPLEMENT_STAGE4="40"
# OLLAMA_TOP_K_OP_DOC_STAGE1="40"
# OLLAMA_TOP_K_OP_DOC_STAGE2="40"
# OLLAMA_TOP_K_OP_EXPLAIN_STAGE1="40"
# OLLAMA_TOP_K_OP_EXPLAIN_STAGE2="40"
# OLLAMA_TOP_K="40"
# OLLAMA_TOP_P_OP_ANNOTATE="0.9"
# OLLAMA_TOP_P_OP_ANNOTATE_POST="0.9"
# OLLAMA_TOP_P_OP_IMPLEMENT_STAGE1="0.9"
# OLLAMA_TOP_P_OP_IMPLEMENT_STAGE2="0.9"
# OLLAMA_TOP_P_OP_IMPLEMENT_STAGE3="0.9"
# OLLAMA_TOP_P_OP_IMPLEMENT_STAGE4="0.9"
# OLLAMA_TOP_P_OP_DOC_STAGE1="0.9"
# OLLAMA_TOP_P_OP_DOC_STAGE2="0.9"
# OLLAMA_TOP_P_OP_EXPLAIN_STAGE1="0.9"
# OLLAMA_TOP_P_OP_EXPLAIN_STAGE2="0.9"
# OLLAMA_TOP_P="0.9"
# OLLAMA_SEED_OP_ANNOTATE="42"
# OLLAMA_SEED_OP_ANNOTATE_POST="42"
# OLLAMA_SEED_OP_IMPLEMENT_STAGE1="42"
# OLLAMA_SEED_OP_IMPLEMENT_STAGE2="42"
# OLLAMA_SEED_OP_IMPLEMENT_STAGE3="42"
# OLLAMA_SEED_OP_IMPLEMENT_STAGE4="42"
# OLLAMA_SEED_OP_DOC_STAGE1="42"
# OLLAMA_SEED_OP_DOC_STAGE2="42"
# OLLAMA_SEED_OP_EXPLAIN_STAGE1="42"
# OLLAMA_SEED_OP_EXPLAIN_STAGE2="42"
# OLLAMA_SEED="42"
# note: values slightly more than 1.0 seem to help against problems when LLM starts to generate repeated content indefinitely, without making report to omit important items
OLLAMA_REPEAT_PENALTY_OP_ANNOTATE="1.1"
OLLAMA_REPEAT_PENALTY_OP_ANNOTATE_POST="1.1"
OLLAMA_REPEAT_PENALTY_OP_IMPLEMENT_STAGE1="1.0"
OLLAMA_REPEAT_PENALTY_OP_IMPLEMENT_STAGE2="1.1"
OLLAMA_REPEAT_PENALTY_OP_IMPLEMENT_STAGE3="1.0"
OLLAMA_REPEAT_PENALTY_OP_IMPLEMENT_STAGE4="1.0"
# OLLAMA_REPEAT_PENALTY_OP_DOC_STAGE1="1.2"
# OLLAMA_REPEAT_PENALTY_OP_DOC_STAGE2="1.2"
OLLAMA_REPEAT_PENALTY_OP_EXPLAIN_STAGE1="1.0"
OLLAMA_REPEAT_PENALTY_OP_EXPLAIN_STAGE2="1.1"
# OLLAMA_REPEAT_PENALTY="1.0"
# OLLAMA_FREQ_PENALTY_OP_ANNOTATE="1.0"
# OLLAMA_FREQ_PENALTY_OP_ANNOTATE_POST="1.0"
# OLLAMA_FREQ_PENALTY_OP_IMPLEMENT_STAGE1="1.0"
# OLLAMA_FREQ_PENALTY_OP_IMPLEMENT_STAGE2="1.0"
# OLLAMA_FREQ_PENALTY_OP_IMPLEMENT_STAGE3="1.0"
# OLLAMA_FREQ_PENALTY_OP_IMPLEMENT_STAGE4="1.0"
# OLLAMA_FREQ_PENALTY_OP_DOC_STAGE1="1.0"
# OLLAMA_FREQ_PENALTY_OP_DOC_STAGE2="1.0"
# OLLAMA_FREQ_PENALTY_OP_EXPLAIN_STAGE1="1.0"
# OLLAMA_FREQ_PENALTY_OP_EXPLAIN_STAGE2="1.0"
# OLLAMA_FREQ_PENALTY="1.0"
# OLLAMA_PRESENCE_PENALTY_OP_ANNOTATE="1.0"
# OLLAMA_PRESENCE_PENALTY_OP_ANNOTATE_POST="1.0"
# OLLAMA_PRESENCE_PENALTY_OP_IMPLEMENT_STAGE1="1.0"
# OLLAMA_PRESENCE_PENALTY_OP_IMPLEMENT_STAGE2="1.0"
# OLLAMA_PRESENCE_PENALTY_OP_IMPLEMENT_STAGE3="1.0"
# OLLAMA_PRESENCE_PENALTY_OP_IMPLEMENT_STAGE4="1.0"
# OLLAMA_PRESENCE_PENALTY_OP_DOC_STAGE1="1.0"
# OLLAMA_PRESENCE_PENALTY_OP_DOC_STAGE2="1.0"
# OLLAMA_PRESENCE_PENALTY_OP_EXPLAIN_STAGE1="1.0"
# OLLAMA_PRESENCE_PENALTY_OP_EXPLAIN_STAGE2="1.0"
# OLLAMA_PRESENCE_PENALTY="1.0"


# Options for generic provider with OpenAI compatible API, JSON structured output mode is not supported
# Below is example for deepseek (https://www.deepseek.com/)

GENERIC_BASE_URL="https://api.deepseek.com/v1" # Required parameter for generic provider (example for deepseek)

# Authentication, optional, but most likely required by your provider, GENERIC_API_KEY="<token>" is now deprecated, will work same as GENERIC_AUTH
# GENERIC_AUTH_TYPE="Bearer" # Type of the authentication used, "Bearer" - api key or token (default), "Basic" - web auth with login and password.
# GENERIC_AUTH="<your api-key or token goes here>" # When using bearer auth type, put your api key or auth token here
# GENERIC_AUTH="<login>:<password>" # Web auth requres login and password separated by a colon

# Some system parameters, provider dependent
# GENERIC_ENABLE_STREAMING="0" # 0 - disabled (default), 1 - enabled, write new data to log right after it generated by LLM, useful for debugging
# GENERIC_MAXTOKENS_FORMAT="old" # values: old, new. Old is "max_tokens=<value>" (default), New is "max_completion_tokens=<value>" (OpenAI using it now)

# General parameters for different operations
# GENERIC_MODEL_OP_ANNOTATE="deepseek-chat"
# GENERIC_MODEL_OP_ANNOTATE_POST="deepseek-chat" # used to process multiple response-variants if any
# GENERIC_MODEL_OP_EMBED="deepseek-r1"
# GENERIC_MODEL_OP_IMPLEMENT_STAGE1="deepseek-chat"
# GENERIC_MODEL_OP_IMPLEMENT_STAGE2="deepseek-chat"
# GENERIC_MODEL_OP_IMPLEMENT_STAGE3="deepseek-chat"
# GENERIC_MODEL_OP_IMPLEMENT_STAGE4="deepseek-chat"
# GENERIC_MODEL_OP_DOC_STAGE1="deepseek-chat"
# GENERIC_MODEL_OP_DOC_STAGE2="deepseek-chat"
# GENERIC_MODEL_OP_EXPLAIN_STAGE1="deepseek-chat"
# GENERIC_MODEL_OP_EXPLAIN_STAGE2="deepseek-chat"
GENERIC_MODEL="deepseek-chat"
# GENERIC_MODEL="deepseek-reasoner" # do not forget to set system prompt role to "user" below
GENERIC_VARIANT_COUNT_OP_ANNOTATE="1" # how much annotate-response variants to generate
GENERIC_VARIANT_SELECTION_OP_ANNOTATE="short" # how to select final variant: short, long, combine, best
GENERIC_VARIANT_COUNT="1" # will be used as fallback
GENERIC_VARIANT_SELECTION="short" # will be used as fallback

# Text chunk/sequence size in characters (not tokens), used when generating embeddings.
# Values too small or too large may lead to less effective search. Highly model-dependent.
# GENERIC_EMBED_CHUNK_SIZE="2048"
# GENERIC_EMBED_CHUNK_OVERLAP="256"

# Cosine score threshold value to consider search vector simiar to target, usually not need to change anything here.
# Model dependent, may be less than 0 for some models, (score < 0 usually means that the vectors are semantically opposite)
# GENERIC_EMBED_SCORE_THRESHOLD="0.0"

# LLM query text prefix used when generating embeddings for project files. Model dependent. Unset by default.
# GENERIC_EMBED_DOC_PREFIX="Process following document:\n"
# LLM query text prefix used when generating embeddings for search queries. Model dependent. Unset by default.
# GENERIC_EMBED_SEARCH_PREFIX="Process following search query:\n"

GENERIC_MAX_TOKENS_OP_ANNOTATE="768"
GENERIC_MAX_TOKENS_OP_ANNOTATE_POST="768"
GENERIC_MAX_TOKENS_OP_IMPLEMENT_STAGE1="512" # file-list for review, long list is probably an error
GENERIC_MAX_TOKENS_OP_IMPLEMENT_STAGE2="1536" # work plan also should not be too big
GENERIC_MAX_TOKENS_OP_IMPLEMENT_STAGE3="512" # file-list for processing, long list is probably an error
GENERIC_MAX_TOKENS_OP_IMPLEMENT_STAGE4="8192" # generated code output limit should be as big as possible, works with DeepSeek
GENERIC_MAX_TOKENS_OP_DOC_STAGE1="768" # file-list for review, longer list is probably an error
GENERIC_MAX_TOKENS_OP_DOC_STAGE2="8192" # generated document output limit should be as big as possible, works with DeepSeek
GENERIC_MAX_TOKENS_OP_EXPLAIN_STAGE1="512" # file-list for review
GENERIC_MAX_TOKENS_OP_EXPLAIN_STAGE2="8192" # generated answer output limit
GENERIC_MAX_TOKENS="4096" # probably should work with most LLMs
GENERIC_MAX_TOKENS_SEGMENTS="3"
GENERIC_ON_FAIL_RETRIES_OP_ANNOTATE="1"
# GENERIC_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE1="3"
# GENERIC_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE2="3"
# GENERIC_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE3="3"
# GENERIC_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE4="3"
# GENERIC_ON_FAIL_RETRIES_OP_DOC_STAGE1="3"
# GENERIC_ON_FAIL_RETRIES_OP_DOC_STAGE2="3"
# GENERIC_ON_FAIL_RETRIES_OP_EXPLAIN_STAGE1="3"
# GENERIC_ON_FAIL_RETRIES_OP_EXPLAIN_STAGE2="3"
GENERIC_ON_FAIL_RETRIES="3"

# Temperature typically affecting LLM behavior the most and it provider dependent, refer your provider's API docs for value limits
# Values provided below seem to work good with DeepSeek
# GENERIC_TEMPERATURE_OP_ANNOTATE="0.5"
# GENERIC_TEMPERATURE_OP_ANNOTATE_POST="0.5"
GENERIC_TEMPERATURE_OP_IMPLEMENT_STAGE1="0.2" # less creative for file-list output
# GENERIC_TEMPERATURE_OP_IMPLEMENT_STAGE2="0.5"
GENERIC_TEMPERATURE_OP_IMPLEMENT_STAGE3="0.2" # less creative for file-list output
# GENERIC_TEMPERATURE_OP_IMPLEMENT_STAGE4="0.5"
GENERIC_TEMPERATURE_OP_DOC_STAGE1="0.2" # less creative for file-list output
GENERIC_TEMPERATURE_OP_DOC_STAGE2="0.7" # more creative when writing documentation
GENERIC_TEMPERATURE_OP_EXPLAIN_STAGE1="0.2" # less creative for file-list output
GENERIC_TEMPERATURE_OP_EXPLAIN_STAGE2="0.7"
GENERIC_TEMPERATURE="0.5"

# System prompt role for model, can be configured per operation. Useful for reasoning models without system prompt, like deepseek-reasoner
# Valid values: system, developer, user. default: system.
# When using "user" role, system prompt will be converted to user-query + ai-acknowledge message sequence
# GENERIC_SYSPROMPT_ROLE_OP_ANNOTATE="system"
# GENERIC_SYSPROMPT_ROLE_OP_ANNOTATE_POST="system"
# GENERIC_SYSPROMPT_ROLE_OP_IMPLEMENT_STAGE1="system"
# GENERIC_SYSPROMPT_ROLE_OP_IMPLEMENT_STAGE2="system"
# GENERIC_SYSPROMPT_ROLE_OP_IMPLEMENT_STAGE3="system"
# GENERIC_SYSPROMPT_ROLE_OP_IMPLEMENT_STAGE4="system"
# GENERIC_SYSPROMPT_ROLE_OP_DOC_STAGE1="system"
# GENERIC_SYSPROMPT_ROLE_OP_DOC_STAGE2="system"
# GENERIC_SYSPROMPT_ROLE_OP_EXPLAIN_STAGE1="system"
# GENERIC_SYSPROMPT_ROLE_OP_EXPLAIN_STAGE2="system"
# GENERIC_SYSPROMPT_ROLE="system"

# Optional regexps for filtering out responses from reasoning models, if LLM provider not doing this automatically
# THINK-regexps will be used to remove reasoning part from response L - is for opening tag, R - is for closing tag
# OUT-regexps will be used to extract output part from response after it was filered out with THINK-regexps
# GENERIC_THINK_RX_L_OP_ANNOTATE="(?mi)^\\s*<think>\\s*\$"
# GENERIC_THINK_RX_R_OP_ANNOTATE="(?mi)^\\s*</think>\\s*\$"
# GENERIC_OUT_RX_L_OP_ANNOTATE="(?mi)^\\s*<output>\\s*\$"
# GENERIC_OUT_RX_R_OP_ANNOTATE="(?mi)^\\s*</output>\\s*\$"
# GENERIC_THINK_RX_L_OP_ANNOTATE_POST="(?mi)^\\s*<think>\\s*\$"
# GENERIC_THINK_RX_R_OP_ANNOTATE_POST="(?mi)^\\s*</think>\\s*\$"
# GENERIC_OUT_RX_L_OP_ANNOTATE_POST="(?mi)^\\s*<output>\\s*\$"
# GENERIC_OUT_RX_R_OP_ANNOTATE_POST="(?mi)^\\s*</output>\\s*\$"
# GENERIC_THINK_RX_L_OP_IMPLEMENT_STAGE1="(?mi)^\\s*<think>\\s*\$"
# GENERIC_THINK_RX_R_OP_IMPLEMENT_STAGE1="(?mi)^\\s*</think>\\s*\$"
# GENERIC_OUT_RX_L_OP_IMPLEMENT_STAGE1="(?mi)^\\s*<output>\\s*\$"
# GENERIC_OUT_RX_R_OP_IMPLEMENT_STAGE1="(?mi)^\\s*</output>\\s*\$"
# GENERIC_THINK_RX_L_OP_IMPLEMENT_STAGE2="(?mi)^\\s*<think>\\s*\$"
# GENERIC_THINK_RX_R_OP_IMPLEMENT_STAGE2="(?mi)^\\s*</think>\\s*\$"
# GENERIC_OUT_RX_L_OP_IMPLEMENT_STAGE2="(?mi)^\\s*<output>\\s*\$"
# GENERIC_OUT_RX_R_OP_IMPLEMENT_STAGE2="(?mi)^\\s*</output>\\s*\$"
# GENERIC_THINK_RX_L_OP_IMPLEMENT_STAGE3="(?mi)^\\s*<think>\\s*\$"
# GENERIC_THINK_RX_R_OP_IMPLEMENT_STAGE3="(?mi)^\\s*</think>\\s*\$"
# GENERIC_OUT_RX_L_OP_IMPLEMENT_STAGE3="(?mi)^\\s*<output>\\s*\$"
# GENERIC_OUT_RX_R_OP_IMPLEMENT_STAGE3="(?mi)^\\s*</output>\\s*\$"
# GENERIC_THINK_RX_L_OP_IMPLEMENT_STAGE4="(?mi)^\\s*<think>\\s*\$"
# GENERIC_THINK_RX_R_OP_IMPLEMENT_STAGE4="(?mi)^\\s*</think>\\s*\$"
# GENERIC_OUT_RX_L_OP_IMPLEMENT_STAGE4="(?mi)^\\s*<output>\\s*\$"
# GENERIC_OUT_RX_R_OP_IMPLEMENT_STAGE4="(?mi)^\\s*</output>\\s*\$"
# GENERIC_THINK_RX_L_OP_DOC_STAGE1="(?mi)^\\s*<think>\\s*\$"
# GENERIC_THINK_RX_R_OP_DOC_STAGE1="(?mi)^\\s*</think>\\s*\$"
# GENERIC_OUT_RX_L_OP_DOC_STAGE1="(?mi)^\\s*<output>\\s*\$"
# GENERIC_OUT_RX_R_OP_DOC_STAGE1="(?mi)^\\s*</output>\\s*\$"
# GENERIC_THINK_RX_L_OP_DOC_STAGE2="(?mi)^\\s*<think>\\s*\$"
# GENERIC_THINK_RX_R_OP_DOC_STAGE2="(?mi)^\\s*</think>\\s*\$"
# GENERIC_OUT_RX_L_OP_DOC_STAGE2="(?mi)^\\s*<output>\\s*\$"
# GENERIC_OUT_RX_R_OP_DOC_STAGE2="(?mi)^\\s*</output>\\s*\$"
# GENERIC_THINK_RX_L_OP_EXPLAIN_STAGE1="(?mi)^\\s*<think>\\s*\$"
# GENERIC_THINK_RX_R_OP_EXPLAIN_STAGE1="(?mi)^\\s*</think>\\s*\$"
# GENERIC_OUT_RX_L_OP_EXPLAIN_STAGE1="(?mi)^\\s*<output>\\s*\$"
# GENERIC_OUT_RX_R_OP_EXPLAIN_STAGE1="(?mi)^\\s*</output>\\s*\$"
# GENERIC_THINK_RX_L_OP_EXPLAIN_STAGE2="(?mi)^\\s*<think>\\s*\$"
# GENERIC_THINK_RX_R_OP_EXPLAIN_STAGE2="(?mi)^\\s*</think>\\s*\$"
# GENERIC_OUT_RX_L_OP_EXPLAIN_STAGE2="(?mi)^\\s*<output>\\s*\$"
# GENERIC_OUT_RX_R_OP_EXPLAIN_STAGE2="(?mi)^\\s*</output>\\s*\$"
# GENERIC_THINK_RX_L="(?mi)^\\s*<think>\\s*\$"
# GENERIC_THINK_RX_R="(?mi)^\\s*</think>\\s*\$"
# GENERIC_OUT_RX_L="(?mi)^\\s*<output>\\s*\$"
# GENERIC_OUT_RX_R="(?mi)^\\s*</output>\\s*\$"

# Remaining options may or may not work, depending on LLM provider
# GENERIC_TOP_K_OP_ANNOTATE="40"
# GENERIC_TOP_K_OP_ANNOTATE_POST="40"
# GENERIC_TOP_K_OP_IMPLEMENT_STAGE1="40"
# GENERIC_TOP_K_OP_IMPLEMENT_STAGE2="40"
# GENERIC_TOP_K_OP_IMPLEMENT_STAGE3="40"
# GENERIC_TOP_K_OP_IMPLEMENT_STAGE4="40"
# GENERIC_TOP_K_OP_DOC_STAGE1="40"
# GENERIC_TOP_K_OP_DOC_STAGE2="40"
# GENERIC_TOP_K_OP_EXPLAIN_STAGE1="40"
# GENERIC_TOP_K_OP_EXPLAIN_STAGE2="40"
# GENERIC_TOP_K="40"
# GENERIC_TOP_P_OP_ANNOTATE="0.9"
# GENERIC_TOP_P_OP_ANNOTATE_POST="0.9"
# GENERIC_TOP_P_OP_IMPLEMENT_STAGE1="0.9"
# GENERIC_TOP_P_OP_IMPLEMENT_STAGE2="0.9"
# GENERIC_TOP_P_OP_IMPLEMENT_STAGE3="0.9"
# GENERIC_TOP_P_OP_IMPLEMENT_STAGE4="0.9"
# GENERIC_TOP_P_OP_DOC_STAGE1="0.9"
# GENERIC_TOP_P_OP_DOC_STAGE2="0.9"
# GENERIC_TOP_P_OP_EXPLAIN_STAGE1="0.9"
# GENERIC_TOP_P_OP_EXPLAIN_STAGE2="0.9"
# GENERIC_TOP_P="0.9"
# GENERIC_SEED_OP_ANNOTATE="42"
# GENERIC_SEED_OP_ANNOTATE_POST="42"
# GENERIC_SEED_OP_IMPLEMENT_STAGE1="42"
# GENERIC_SEED_OP_IMPLEMENT_STAGE2="42"
# GENERIC_SEED_OP_IMPLEMENT_STAGE3="42"
# GENERIC_SEED_OP_IMPLEMENT_STAGE4="42"
# GENERIC_SEED_OP_DOC_STAGE1="42"
# GENERIC_SEED_OP_DOC_STAGE2="42"
# GENERIC_SEED_OP_EXPLAIN_STAGE1="42"
# GENERIC_SEED_OP_EXPLAIN_STAGE2="42"
# GENERIC_SEED="42"
# GENERIC_REPEAT_PENALTY_OP_ANNOTATE="1.2"
# GENERIC_REPEAT_PENALTY_OP_ANNOTATE_POST="1.2"
# GENERIC_REPEAT_PENALTY_OP_IMPLEMENT_STAGE1="1.2"
# GENERIC_REPEAT_PENALTY_OP_IMPLEMENT_STAGE2="1.5"
# GENERIC_REPEAT_PENALTY_OP_IMPLEMENT_STAGE3="1.0"
# GENERIC_REPEAT_PENALTY_OP_IMPLEMENT_STAGE4="1.0"
# GENERIC_REPEAT_PENALTY_OP_DOC_STAGE1="1.2"
# GENERIC_REPEAT_PENALTY_OP_DOC_STAGE2="1.2"
# GENERIC_REPEAT_PENALTY_OP_EXPLAIN_STAGE1="1.0"
# GENERIC_REPEAT_PENALTY_OP_EXPLAIN_STAGE2="1.1"
# GENERIC_REPEAT_PENALTY="1.1"
# GENERIC_FREQ_PENALTY_OP_ANNOTATE="1.0"
# GENERIC_FREQ_PENALTY_OP_ANNOTATE_POST="1.0"
# GENERIC_FREQ_PENALTY_OP_IMPLEMENT_STAGE1="1.0"
# GENERIC_FREQ_PENALTY_OP_IMPLEMENT_STAGE2="1.0"
# GENERIC_FREQ_PENALTY_OP_IMPLEMENT_STAGE3="1.0"
# GENERIC_FREQ_PENALTY_OP_IMPLEMENT_STAGE4="1.0"
# GENERIC_FREQ_PENALTY_OP_DOC_STAGE1="1.0"
# GENERIC_FREQ_PENALTY_OP_DOC_STAGE2="1.0"
# GENERIC_FREQ_PENALTY_OP_EXPLAIN_STAGE1="1.0"
# GENERIC_FREQ_PENALTY_OP_EXPLAIN_STAGE2="1.0"
# GENERIC_FREQ_PENALTY="1.0"
# GENERIC_PRESENCE_PENALTY_OP_ANNOTATE="1.0"
# GENERIC_PRESENCE_PENALTY_OP_ANNOTATE_POST="1.0"
# GENERIC_PRESENCE_PENALTY_OP_IMPLEMENT_STAGE1="1.0"
# GENERIC_PRESENCE_PENALTY_OP_IMPLEMENT_STAGE2="1.0"
# GENERIC_PRESENCE_PENALTY_OP_IMPLEMENT_STAGE3="1.0"
# GENERIC_PRESENCE_PENALTY_OP_IMPLEMENT_STAGE4="1.0"
# GENERIC_PRESENCE_PENALTY_OP_DOC_STAGE1="1.0"
# GENERIC_PRESENCE_PENALTY_OP_DOC_STAGE2="1.0"
# GENERIC_PRESENCE_PENALTY_OP_EXPLAIN_STAGE1="1.0"
# GENERIC_PRESENCE_PENALTY_OP_EXPLAIN_STAGE2="1.0"
# GENERIC_PRESENCE_PENALTY="1.0"
# GENERIC_REASONING_EFFORT_OP_ANNOTATE="medium" # values: low, medium, high
# GENERIC_REASONING_EFFORT_OP_ANNOTATE_POST="medium"
# GENERIC_REASONING_EFFORT_OP_IMPLEMENT_STAGE1="medium"
# GENERIC_REASONING_EFFORT_OP_IMPLEMENT_STAGE2="medium"
# GENERIC_REASONING_EFFORT_OP_IMPLEMENT_STAGE3="medium"
# GENERIC_REASONING_EFFORT_OP_IMPLEMENT_STAGE4="medium"
# GENERIC_REASONING_EFFORT_OP_DOC_STAGE1="medium"
# GENERIC_REASONING_EFFORT_OP_DOC_STAGE2="medium"
# GENERIC_REASONING_EFFORT_OP_EXPLAIN_STAGE1="medium"
# GENERIC_REASONING_EFFORT_OP_EXPLAIN_STAGE2="medium"
# GENERIC_REASONING_EFFORT="medium"
