# Implement Operation

The `implement` operation is a main feature of the Perpetual tool designed to automate code implementation based on user-provided instructions. This operation leverages Large Language Models (LLMs) to analyze your project, understand the context, and generate or modify code according to your specifications.

## Understanding the Implement Operation

The `implement` operation works by identifying and processing sections of your code marked with `###IMPLEMENT###` comments. These comments serve as indicators for where new code should be generated or existing code should be modified. The operation follows a multi-stage process to ensure accurate and contextually appropriate code implementation:

1. **Project Analysis**: The operation begins by analyzing your project structure and content. It utilizes the project index and annotations generated by the `annotate` operation to understand the overall context of your codebase.

2. **Target File Identification**: Files containing `###IMPLEMENT###` comments are identified as targets for code implementation.

3. **Context Gathering**: The operation collects relevant information from the target files and related project files to provide comprehensive context to the LLM.

4. **Code Generation**: Using the gathered context and the instructions provided in the `###IMPLEMENT###` comments, the LLM generates or modifies code for each target file. It also modifies related files if needed or can even create new files.

5. **Integration**: The generated code is seamlessly integrated into your project, replacing the `###IMPLEMENT###` comments and/or modifying other existing code as specified.

Throughout this process, the `implement` operation relies heavily on the project index and file annotations to make informed decisions about code implementation. This ensures that the generated code is consistent with your project's structure, coding style, and existing functionality.

## Workflow and Usage Guide

To effectively use the `implement` operation, follow this typical workflow:

1. **Project Setup**:
   - Create the basic structure of your project, including main files and directories.
   - Initialize your project for use with Perpetual tool by using `init` operation.
   - Create local `.env` configuration file at `<project_root>/.perpetual/.env` and/or global configuration file at `~/.config/Perpetual/.env` on Linux, `<User profile dir>\AppData\Roaming\Perpetual\.env` on Windows. Settings from the local project configuration file will take precedence over global configuration settings.

2. **Marking Implementation Points**:
   - In your source files, use `###IMPLEMENT###` comments to indicate where you want code to be generated or modified.
   - Example:

     ```go
     //###IMPLEMENT###
     //Create a function to check user input
     ```

3. **Running the Implement Operation**:
   - Execute the implement operation using the command:

     ```sh
     Perpetual implement [flags]
     ```

   - The operation will process all files with `###IMPLEMENT###` comments.

4. **Reviewing and Iterating**:
   - Review the generated code for accuracy and consistency.
   - If necessary, use the `stash` operation to revert changes:

     ```sh
     Perpetual stash -r
     ```

   - Modify your `###IMPLEMENT###` comments to provide more specific instructions if needed.
   - Re-run the `implement` operation to generate new code based on updated instructions.

5. **Finalizing**:
   - Once satisfied with the generated code, commit the changes to your version control system.
   - Repeat from step 2 for further implementations.

### Special Comments

- `###IMPLEMENT###`: Marks sections for code implementation. You can provide detailed instructions after this comment.
- `###NOUPLOAD###`: Place this comment at the top of files containing sensitive or unneeded information. Files with this comment will not be sent to the LLM for processing on `implement` operation. **This is not meant for privacy**, but to reduce LLM context clogging with unrelated code. To ensure the file will never be processed by LLM, use `project_files_blacklist.json` (see the `init` operation).

It's important to note that while the `###NOUPLOAD###` comment prevents the full file content from being sent to the LLM during the `implement` operation, it does not provide complete protection against data exposure. The file will still be processed during the `annotate` operation, which may use a local LLM for generating annotations. This annotation process is necessary to create the project index, which helps the LLM understand the project structure and write new code in context. While the annotation may leak some contextual information about the file, this can be mitigated with special summarization instructions (see the `annotate` operation documentation for more details). Users should be aware of these limitations and take appropriate precautions when dealing with sensitive information.

### Examining logs

`Perpetual` provides detailed logging of LLM interaction at `<project_root>/.perpetual/.message_log.txt` file: This file contains unformatted log of the actual messages exchanged between `Perpetual` and the LLM. This log provides a complete record of the communication, including any repeated messages, and can be useful if you need to understand the exact content of the messages sent to the LLM.

## Command-Line Usage

To run the `implement` operation, use the following command:

```sh
Perpetual implement [flags]
```

Supported flags:

- `-h`: Display help information about the `implement` operation.
- `-n`: No annotate mode. Skip re-annotating changed files and use current annotations if any.
- `-p`: Enable extended planning stage. Useful for larger modifications that may create new files. Disabled by default to save tokens.
- `-pr`: Enables planning with additional reasoning. May produce improved results for complex or abstractly described tasks, but can also lead to flawed reasoning and worsen the final outcome. This flag includes the `-p` flag.
- `-r <file>`: Manually request a specific file for the operation. If not specified, files are selected automatically.
- `-v`: Enable debug logging for more detailed output.
- `-vv`: Enable both debug and trace logging for maximum verbosity.
- `-t`: Exclude unit-tests source files from being processed.
- `-x <file>`: Path to user-supplied regex filter-file for filtering out certain files from processing. (use `project_test_files_blacklist.json` or `project_files_blacklist.json` inside `<project_root>/.perpetual` directory as reference)

## Configuration

The `implement` operation can be fine-tuned using environment variables in the `.env` file. These variables allow you to customize the behavior of the LLM used for code implementation. Key configuration options include:

1. **LLM Provider**:
   - `LLM_PROVIDER_OP_IMPLEMENT_STAGE1`, `LLM_PROVIDER_OP_IMPLEMENT_STAGE2`, `LLM_PROVIDER_OP_IMPLEMENT_STAGE3`: Specify the LLM provider for each stage of the implement operation.

2. **Model Selection**:
   - `ANTHROPIC_MODEL_OP_IMPLEMENT_STAGE1`, `ANTHROPIC_MODEL_OP_IMPLEMENT_STAGE2`, `ANTHROPIC_MODEL_OP_IMPLEMENT_STAGE3`: Anthropic models for each stage.
   - Similar variables exist for OpenAI and Ollama providers (e.g `OPENAI_MODEL_OP_IMPLEMENT_STAGE1`, `OLLAMA_MODEL_OP_IMPLEMENT_STAGE1` etc)

3. **Token Limits**:
   - `ANTHROPIC_MAX_TOKENS_OP_IMPLEMENT_STAGE1`, `ANTHROPIC_MAX_TOKENS_OP_IMPLEMENT_STAGE2`, `ANTHROPIC_MAX_TOKENS_OP_IMPLEMENT_STAGE3`: Set maximum tokens for each stage.

4. **Retry Settings**:
   - `ANTHROPIC_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE1`, `ANTHROPIC_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE2`, `ANTHROPIC_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE3`: Specify retry attempts for each stage.

5. **Temperature**:
   - `ANTHROPIC_TEMPERATURE_OP_IMPLEMENT_STAGE1`, `ANTHROPIC_TEMPERATURE_OP_IMPLEMENT_STAGE2`, `ANTHROPIC_TEMPERATURE_OP_IMPLEMENT_STAGE3`: Set temperature for each stage.

6. **Other LLM Parameters**:
   - `TOP_K`, `TOP_P`, `SEED`, `REPEAT_PENALTY`, `FREQ_PENALTY`, `PRESENCE_PENALTY`: Can be set for each stage by appending `_OP_IMPLEMENT_STAGE1`, `_OP_IMPLEMENT_STAGE2`, or `_OP_IMPLEMENT_STAGE3`.

Example configuration in `.env` file:

```sh
LLM_PROVIDER="anthropic"
ANTHROPIC_MODEL_OP_IMPLEMENT_STAGE1="claude-3-haiku-20240307"
ANTHROPIC_MODEL_OP_IMPLEMENT_STAGE2="claude-3-5-sonnet-20240620"
ANTHROPIC_MODEL_OP_IMPLEMENT_STAGE3="claude-3-5-sonnet-20240620"
ANTHROPIC_MAX_TOKENS_OP_IMPLEMENT_STAGE1="4096"
ANTHROPIC_MAX_TOKENS_OP_IMPLEMENT_STAGE2="4096"
ANTHROPIC_MAX_TOKENS_OP_IMPLEMENT_STAGE3="4096"
ANTHROPIC_TEMPERATURE_OP_IMPLEMENT_STAGE1="0.5"
ANTHROPIC_TEMPERATURE_OP_IMPLEMENT_STAGE2="0.5"
ANTHROPIC_TEMPERATURE_OP_IMPLEMENT_STAGE3="0.5"
ANTHROPIC_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE1="3"
ANTHROPIC_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE2="3"
ANTHROPIC_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE3="3"
```

This configuration uses the Anthropic provider with the Claude 3.5 Sonnet model for stages 2 and 3, and the Claude 3 Haiku model for stage 1, where it collects context for stages 2 and 3 using the cheaper model to lower costs. It sets a maximum of 4096 tokens, uses a temperature of 0.5, and allows up to 3 retries on failure for each stage. As of the date of writing this document, Claude 3.5 Sonnet is recommended for all stages, but in the future, it may be better to set an even more powerful model for stage 3 when it becomes available.

## Best Practices

To get the most out of the `implement` operation, consider these best practices:

1. **Clear Instructions**: Provide detailed and clear instructions in your `###IMPLEMENT###` comments. The more specific you are, the better the generated code will be.

2. **Incremental Implementation**: For complex features, break down the implementation into smaller, manageable tasks. This allows for easier review and iteration.

3. **Regular Code Reviews**: Always review the generated code carefully. While the LLM is powerful, it may not always produce perfect code on the first try.

4. **Version Control**: Use version control systems to track changes and easily revert if necessary. The `stash` operation can also help with this.

5. **Consistent Coding Style**: Ensure your project has a consistent coding style. The LLM will attempt to match the style of existing code, so maintaining consistency helps produce better results.

6. **Maintaining Good Project Architecture**: The better and easier to understand and maintain your architecture is, the better results the LLM will provide. Use S.O.L.I.D. principles, split your code into smaller and more specialized units. Place each unit into separate files.

7. **Use Planning Flags**: For complex implementations that may require creating new files or extensive changes, use the `-p` or `-pr` flags to enable more thorough planning.

8. **Use of `###NOUPLOAD###` comment**: Use the `###NOUPLOAD###` comment in files containing sensitive information to prevent them from being directly processed by the LLM in the `implement` operation. Be aware of its limitations as described earlier in this document. For complete exclusion of files from processing, use `project_files_blacklist.json` (see the `init` operation).

   The `###NOUPLOAD###` comment can also be used to prevent the LLM context from being clogged with unnecessary information. For example, you may want to avoid uploading implementations of some interfaces when only the interface is sufficient to implement the task, or some large files with text constants to lower the risk of the LLM treating it as direct instructions. This is especially useful when using LLMs with smaller context windows.

9. **Iterative Refinement**: If the initial implementation isn't satisfactory, refine your `###IMPLEMENT###` comments and re-run the operation. Each iteration can bring you closer to the desired result.

10. **Fine-tune LLM Settings**: Experiment with different LLM settings in your `.env` file to find the configuration that works best for your project and coding style. See `.env.example` file at `<project_root>/.perpetual/.env.example` for all config options.

## Implementation Details

The `implement` operation is divided into three main stages.

### Stage 1: Context Gathering

This stage is responsible for analyzing the project and gathering context for the implementation. It performs the following tasks:

1. Run `annotate` to update project source-code files annotations.
2. Generates a project-index containing file names and their annotations.
3. Creates a request for files with `###IMPLEMENT###` comments. It queries the LLM to identify what project source-code files are relevant for the implementation according to the project index.
4. Returns the list of files to review.

### Stage 2: Planning

This stage plans the implementation based on the context gathered in Stage 1. It includes:

1. Gather source code from relevant project files requested by LLM at stage 1, that are needed to implement requested code.
2. Querying the LLM to determine which files will be modified or created as a result of implementing code.
3. Processing the LLM's response to extract file-list and reasoning (if enabled with `-pr` flag).

### Stage 3: Code Generation

This final stage generates the actual code based on the planning from Stage 2. It includes:

1. Gather source code from relevant project files requested by LLM at stage 1, that are needed to implement requested code. Also, if available, use reasonings extracted from stage 2 as further instructions.
2. Iteratively processing each file that needs modification or creation.
3. Querying the LLM to produce the implemented code.
4. Handling partial responses and continuing generation if token limits are reached.
5. Parsing and storing the generated code for each file.

## Error Handling and Retries

The `implement` operation includes robust error handling and retry mechanisms to ensure reliable code generation:

1. **LLM Query Failures**: If an LLM query fails, the operation will retry up to the number of times specified in the `<PROVIDER>_ON_FAIL_RETRIES_OP_IMPLEMENT_STAGE<NUMBER>` environment variables.

2. **Token Limit Handling**: If the LLM response reaches the token limit, the operation attempts to continue generating code from where it left off. This is particularly useful for large files or complex implementations, but the result heavily depends on the LLM's ability to follow the instruction. Currently (as of September 2024) works best with Anthropic Claude 3.5 Sonnet model, and may also work well with GPT-4 or GPT-4o model.

3. **Invalid Responses**: The operation checks for properly formatted code-blocks in the LLM responses (by default it tries to detect MD formatted code-blocks, but you may customize it for any other format). If no valid code is found, it will retry the query. Currently, there are no plans for using so-called `JSON-mode`, or other similar fancy features of leading LLM providers and models - because it's not so universal and even provides worse results, at least for now. This may change in the future.

4. **File Processing Errors**: If there's an error processing a specific file, the operation logs the error and continues with the next file, ensuring that a single file doesn't halt the entire process.

## Performance Considerations

The `implement` operation can take a lot of time (when using locally running LLM) or money when using commercial LLM providers, especially for large projects or complex implementations. Consider the following to optimize performance:

1. **Use Appropriate Models**: Choose LLM models/providers that balance between capability and speed. For example, using a smaller model for Stage 1 and more powerful models for Stages 2 and 3. You may also try using small local models with Ollama for `annotate` operation, this will save you some money on auto re-annotating of changed files.

2. **Do not use `-p` or `-pr` flags unless needed**: You may significantly save on LLM API calls, tokens, and costs by not using these flags, if you know that changes won't produce any new files, or not trigger rewrites in other files not marked with `###IMPLEMENT###` comments.

3. **Incremental Implementation**: For large projects, implement changes in smaller, manageable chunks rather than attempting to modify the entire codebase at once.

4. **Use the `-t` flag**: If your project contains unit tests that are not relevant to the implementation task, use the `-t` flag to exclude them from processing. This can significantly reduce the amount of code the LLM needs to analyze, improving quality of generated code and reducing costs at the same time.

5. **Custom File Filtering**: For more fine-grained control over which files are processed, use the `-x` flag with a custom regex filter file. This allows you to exclude specific files or file types that are not relevant to your current implementation task, reducing your costs.
